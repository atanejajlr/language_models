<html>
<head>
  <title>Insight into the Decision Tree Model</title>
  <style>
    body {
      margin: 0 auto;
      width: 744px;
      font-family: Source Serif Pro, serif;
      line-height: 32px;
      font-weight: 400;
      color: rgba(0, 0, 0, 0.7);
      font-size: 21px;
    }
    h1, h2, h3 {
      font-family: Source Sans Pro, Helvetica, Arial, sans-serif;
    }
    h1 a, h1 a:visited {
      color: inherit;
      text-decoration: none;
    }
    h1 {
      line-height: 48px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 42px;
      margin: 32px 0 20px;
    }
    h2 {
      line-height: 32px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 26px;
      margin: 28px 0;
    }
    h3 {
      line-height: 28px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 21px;
      margin: 24px 0;
    }
    p {
      margin: 32px 0;
    }
    .created, .published {
      color: rgba(0, 0, 0, 0.55);
      font-size: 15px;
      line-height: 15px;
      margin: 20px 0;
    }
    .created + .published {
      margin-top: -12px;
    }
    blockquote {
      font-family: Georgia, Source Serif Pro, serif;
      font-style: italic;
      font-size: 24px;
      line-height: 36px;
      margin: 48px 120px;
      text-align: center;
    }
    a {
      word-wrap: break-word;
      outline: none;
      text-decoration: none;
      background-color: transparent;
      border: 0;
      color: #008CC9;
    }
    a:hover {
      text-decoration: underline;
    }
    a:visited {
      color: #8C68CB;
    }
    .center {
      text-align: center;
    }
    iframe {
      display: block;
      margin: 44px auto;
    }
    *:not(pre) + pre, pre:first-of-type {
      margin-top: 32px;
      padding-top: 32px;
    }
    pre:only-of-type {
      margin: 32px 0;
      padding: 32px;
    }
    pre {
      background: #F3F6F8;
      overflow-x: auto;
      display: block;
      font-size: 13px;
      font-family: monospace;
      line-height: 13px;
      padding: 0 32px 32px;
      white-space: pre;
    }
    a.embedded {
      background: #F3F6F8;
      display: block;
      padding: 32px;
      margin: 32px 0;
    }
    img {
      height: auto;
      max-width: 100%;
    }
    .slate-image-embed__resize-full-width img {
      width: 100%;
    }
    .series-logo {
      width: 48px;
      height: 48px;
      box-sizing: border-box;
      background-clip: content-box;
      border: 4px solid transparent;
      border-radius: 6px;
      object-fit: scale-down;
      float: left;
    }
    .series-title {
      font-size: 16px;
      font-weight: 600;
      vertical-align: top;
    }
    .series-description {
      color: rgba(0,0,0,.6);
      font-weight: 400;
      font-size: 14px;
      line-height: 20px;
    }
    div {
      margin: 32px 0;
    }
  </style>
</head>
<body>
    <img src="https://media.licdn.com/mediaD4E12AQHU4Adrq5wV3Q" alt="" title="" />
      <h1><a href="https://www.linkedin.com/pulse/insight-decision-tree-model-ajay-taneja">Insight into the Decision Tree Model</a></h1>
    <p class="created">Created on 2021-10-26 18:32</p>
  <p class="published">Published on 2023-01-27 22:37</p>
  <div><h2>1.&nbsp;&nbsp;&nbsp;Introduction</h2><p>&nbsp;In my blog of some months ago, titled ‚ÄúThe Strength of Weak Learnability‚Äù and documented on LinkedIn here[<a href="https://www.linkedin.com/pulse/adaboostadaptive-boosting-gradient-xgboost-extreme-ajay-taneja/" target="_blank">https://www.linkedin.com/pulse/adaboostadaptive-boosting-gradient-xgboost-extreme-ajay-taneja/</a>], I have talked of different machine learning algorithms based on the concept of ‚ÄúBoosting‚Äù i.e. combining several ‚Äúweak learners‚Äù to form an overall strong learning algorithm‚Äù. Some of these algorithms that I had discussed in quite some detail included: </p><ul><li>Random Forest Algorithm which randomly samples from the data set and uses each of the sample (each sample normally being of the same size N) for training each decision tree. Additionally, Random Forests, we randomly select / sample the features for each tree.</li></ul><p><br></p><ul><li>In Adaptive Boosting the overall model split into N decision trees (precisely N weak models ‚Äì let us consider that the weak learner is a decision tree) ‚Äì each decision tree is a decision stump (see figure below) with a single split ‚Äì N here refers to the number of features. Thus, we create a decision stump for every feature and the first decision stump will be the one with the lowest measure of randomness (Entropy / Gini Index).</li></ul><p>&nbsp;</p><ul><li>Gradient Boosting and XGBoost ‚Äì wherein, in Gradient Boosting the model learns the weights through the gradient. The gradient is used to minimize the loss function and ‚Äúlearns‚Äù the weights ‚Äì here the model learns to assign higher weights to the misclassified samples. Whereas, in XGBoost, the aim is to make the process computationally efficient through distributed computing</li></ul><p>&nbsp;Since all of the approaches employ decision trees, I felt it necessary to discuss about the concepts involving Entropy and Information Gain related to Decision Trees. These are discussed in the subsequent sections</p><h2>2. Building a Decision Tree Model: An insight into Entropy and Information Gain</h2><p><strong> Measure of impurity in a Decision Tree model:</strong></p><p> Whilst dealing with Decision trees, we need to look at a way of measuring the purity of all the examples. For example, if you're dealing with a classification problem of classifying a set of images of specimen as cracked or uncracked and we classify a set as all cracked (or, all uncracked) then it's very pure but if we classify 50% of examples as cracked and remaining 50% as uncracked then it is not pure. How do we quantify the measure of purity here?</p><p><strong>Entropy</strong></p><p> We're going to measure the "impurity" by a function called "entropy". Let "p1" denote the fraction of cracked to uncracked specimen. Then, if p1 = 3/ 6 then, the entropy - which is defined the measure of "impurity" is 1. That is, the inverse, measure of purity is 0. This is because there is a lot of randomness when using that feature as a split.</p><p> </p><p> On the other hand, if all the specimens are classified as cracked (or, uncracked) measure of impurity will be 0 and measure of purity is thus 1.</p><p> </p><p> It should be noticed that the curve of entropy is highest when p1 is 0.5 and at 0 when p1 is 1. This function of entropy is defined as shown in the figure below.</p><p> </p><p> <strong>Why the above discussion is important: Choosing a split?</strong></p><p><br></p><p>The above discussion is important because, when building a decision tree, this way we will decide what feature to split on at a node. We must choose a feature which results in minimum entropy or otherwise: maximum information gain.</p><p> So, supposing consider the above classification problem of classify ing the specimen as cracked or uncracked, we choose a feature say the root radius as less than x mm or greater than x mm. And compute p1 for each of these branches as p1=4/5 and p1=1/5 respectively and thus the entropy: H(4/5) = 0.72 and H(1/5) = 0.72</p><p>We first calculate the weighted average of each of the sub branches , the weighted average being the number of examples of the total examples in each of the sub branch</p><p>Now, we do not use the entropy directly to choose the split. We choose the reduction in entropy which is H(0.5) minus the weighted average above.</p><p> This is called as "Information Gain" which actually measures the reduction in entropy.</p><p> And one reason for taking the Information Gain (reduction in entropy) for choosing the best split is that one can make sure to not increase the size of the tree unnecessarily thus risking overfitting when there's no reduction in entropy.</p><figure><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4E12AQHrFqS7Wv7l6g" src="https://media.licdn.com/dms/image/D4E12AQHrFqS7Wv7l6g/article-inline_image-shrink_1000_1488/0/1674858483175?e=1691625600&amp;v=beta&amp;t=rXQHdsf_wBy_Gl1N3z98GTwuA15Yl7SF7RB-iAh2qQM"></figure><h2>3. Steps in Decision Tree Model Build</h2><p><br></p><p>Having discussed the above, let us now list the step by step process in Decision Tree model build. These are the steps will have to carry out when building a Decision tree model from the scratch:</p><p> </p><ol><li> Start with all the examples at the root node</li><li> Calculate the information gain for all the features and select the feature that gives the maximum information gain</li><li> Split the dataset into left and right branches.</li><li> Continue the process through all the features until one of the criteria is reached:</li></ol><p> </p><p> a) You get all examples of a single class following a split</p><p> b) You have reached a maximum depth of the tree</p><p> c) You do not get any further information gain by growing the tree further</p><p> d) The number if remaining examples following a split are below a certain threshold</p><p> </p><p>Following points should be noted:</p><p><br></p><p> üëâ Growing a tree to a higher depth is analogous to fitting a higher degree polynomial which will result in learning the training data well but not performing well on unseen data i.e. overfitting</p><p> üëâThe maximum depth of the tree is normally taken as the default value from whatever library you're using</p><p> üëâWhen building a decision tree from the scratch, one would normally use a recursive algorithm</p><p> </p><p> </p></div>
</body>
</html>