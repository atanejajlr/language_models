<html>
<head>
  <title>SeqToSeq Models vs Attention Models: A point of view</title>
  <style>
    body {
      margin: 0 auto;
      width: 744px;
      font-family: Source Serif Pro, serif;
      line-height: 32px;
      font-weight: 400;
      color: rgba(0, 0, 0, 0.7);
      font-size: 21px;
    }
    h1, h2, h3 {
      font-family: Source Sans Pro, Helvetica, Arial, sans-serif;
    }
    h1 a, h1 a:visited {
      color: inherit;
      text-decoration: none;
    }
    h1 {
      line-height: 48px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 42px;
      margin: 32px 0 20px;
    }
    h2 {
      line-height: 32px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 26px;
      margin: 28px 0;
    }
    h3 {
      line-height: 28px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 21px;
      margin: 24px 0;
    }
    p {
      margin: 32px 0;
    }
    .created, .published {
      color: rgba(0, 0, 0, 0.55);
      font-size: 15px;
      line-height: 15px;
      margin: 20px 0;
    }
    .created + .published {
      margin-top: -12px;
    }
    blockquote {
      font-family: Georgia, Source Serif Pro, serif;
      font-style: italic;
      font-size: 24px;
      line-height: 36px;
      margin: 48px 120px;
      text-align: center;
    }
    a {
      word-wrap: break-word;
      outline: none;
      text-decoration: none;
      background-color: transparent;
      border: 0;
      color: #008CC9;
    }
    a:hover {
      text-decoration: underline;
    }
    a:visited {
      color: #8C68CB;
    }
    .center {
      text-align: center;
    }
    iframe {
      display: block;
      margin: 44px auto;
    }
    *:not(pre) + pre, pre:first-of-type {
      margin-top: 32px;
      padding-top: 32px;
    }
    pre:only-of-type {
      margin: 32px 0;
      padding: 32px;
    }
    pre {
      background: #F3F6F8;
      overflow-x: auto;
      display: block;
      font-size: 13px;
      font-family: monospace;
      line-height: 13px;
      padding: 0 32px 32px;
      white-space: pre;
    }
    a.embedded {
      background: #F3F6F8;
      display: block;
      padding: 32px;
      margin: 32px 0;
    }
    img {
      height: auto;
      max-width: 100%;
    }
    .slate-image-embed__resize-full-width img {
      width: 100%;
    }
    .series-logo {
      width: 48px;
      height: 48px;
      box-sizing: border-box;
      background-clip: content-box;
      border: 4px solid transparent;
      border-radius: 6px;
      object-fit: scale-down;
      float: left;
    }
    .series-title {
      font-size: 16px;
      font-weight: 600;
      vertical-align: top;
    }
    .series-description {
      color: rgba(0,0,0,.6);
      font-weight: 400;
      font-size: 14px;
      line-height: 20px;
    }
    div {
      margin: 32px 0;
    }
  </style>
</head>
<body>
    <img src="https://media.licdn.com/mediaC4D12AQHpC_Bo2JbwEg" alt="" title="" />
      <h1><a href="https://www.linkedin.com/pulse/seqtoseq-models-vs-attention-point-view-ajay-taneja">SeqToSeq Models vs Attention Models: A point of view</a></h1>
    <p class="created">Created on 2021-02-19 02:22</p>
  <p class="published">Published on 2021-02-19 02:42</p>
  <div><p>This is just a (spontaneous!) point of view that I'm expressing in this blog post as I had been doing some reading and courses involving Natural Language Processing since a year and in particular the highly popular 'SeqtoSeq Models' and 'Attention Models' since some weeks!</p><p>Sequence to Sequence (many times termed as Seq2Seq) models are a class of Recurrent Neural Network (RNN) architectures typically used to solve complex Language related problems like Machine Translation, Question Answering, Chat-bots, Text Summarization, etc.</p><p>Firstly, why the traditional SeqtoSeq (2014, Google) model do not perform well in certain cases? This is because they fail to cater well to Long sequence of sentences (e.g. consider the use case involving machine translation). They have a fixed size vector approach wherein all the information coming from hidden states of the encoder gets compressed onto the fixed sized vector. This suffers a drawback wherein, if the sequence of sentences / text that is processed through the encoder is long, then, this results in some information “getting lost” – more precisely the information towards the beginning of the sentence is lost and emphasis being more towards the end of the sentence</p><p>There comes the “Attention Model” wherein, for every hidden state of the decoder we compute what is called as “attention weights”. The attention weights essentially are the weightages “from” every hidden state in the encoder “on” every hidden state of the decoder. This results in having a measured contribution from every state of the encoder than just “randomly” having a fixed length vector.To calculate the attention weights, we proceed as follows;</p><p>We calculate the attention scores, first. The inputs for the calculation of attention scores being a) all hidden states of the encoder&nbsp;b) the previous hidden state of the decoder. The above inputs are passed through a neural network. Having calculated the attention scores, these are passed through a softmax to convert these scores into probabilities between 0 and 1. These probabilities multiplied with the hidden states of the encoder (respectively) gives the context vector which becomes an input the particular decoder unit. It should be underscored, the attention weights have to be calculated for "every" decoder unit. Thus, every decoder unit will have its own set of attention weights. This whole process results in having "the necessary" information available to the decoder which is not the case in SeqtoSeq models.</p><p>The scores computation can be obtained through a small by a small neural network and the whole model can then be optimised using any gradient optimisation method such as gradient descent.</p></div>
</body>
</html>