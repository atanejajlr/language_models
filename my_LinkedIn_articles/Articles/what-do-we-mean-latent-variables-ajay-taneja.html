<html>
<head>
  <title>Deep Generative Models </title>
  <style>
    body {
      margin: 0 auto;
      width: 744px;
      font-family: Source Serif Pro, serif;
      line-height: 32px;
      font-weight: 400;
      color: rgba(0, 0, 0, 0.7);
      font-size: 21px;
    }
    h1, h2, h3 {
      font-family: Source Sans Pro, Helvetica, Arial, sans-serif;
    }
    h1 a, h1 a:visited {
      color: inherit;
      text-decoration: none;
    }
    h1 {
      line-height: 48px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 42px;
      margin: 32px 0 20px;
    }
    h2 {
      line-height: 32px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 26px;
      margin: 28px 0;
    }
    h3 {
      line-height: 28px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 21px;
      margin: 24px 0;
    }
    p {
      margin: 32px 0;
    }
    .created, .published {
      color: rgba(0, 0, 0, 0.55);
      font-size: 15px;
      line-height: 15px;
      margin: 20px 0;
    }
    .created + .published {
      margin-top: -12px;
    }
    blockquote {
      font-family: Georgia, Source Serif Pro, serif;
      font-style: italic;
      font-size: 24px;
      line-height: 36px;
      margin: 48px 120px;
      text-align: center;
    }
    a {
      word-wrap: break-word;
      outline: none;
      text-decoration: none;
      background-color: transparent;
      border: 0;
      color: #008CC9;
    }
    a:hover {
      text-decoration: underline;
    }
    a:visited {
      color: #8C68CB;
    }
    .center {
      text-align: center;
    }
    iframe {
      display: block;
      margin: 44px auto;
    }
    *:not(pre) + pre, pre:first-of-type {
      margin-top: 32px;
      padding-top: 32px;
    }
    pre:only-of-type {
      margin: 32px 0;
      padding: 32px;
    }
    pre {
      background: #F3F6F8;
      overflow-x: auto;
      display: block;
      font-size: 13px;
      font-family: monospace;
      line-height: 13px;
      padding: 0 32px 32px;
      white-space: pre;
    }
    a.embedded {
      background: #F3F6F8;
      display: block;
      padding: 32px;
      margin: 32px 0;
    }
    img {
      height: auto;
      max-width: 100%;
    }
    .slate-image-embed__resize-full-width img {
      width: 100%;
    }
    .series-logo {
      width: 48px;
      height: 48px;
      box-sizing: border-box;
      background-clip: content-box;
      border: 4px solid transparent;
      border-radius: 6px;
      object-fit: scale-down;
      float: left;
    }
    .series-title {
      font-size: 16px;
      font-weight: 600;
      vertical-align: top;
    }
    .series-description {
      color: rgba(0,0,0,.6);
      font-weight: 400;
      font-size: 14px;
      line-height: 20px;
    }
    div {
      margin: 32px 0;
    }
  </style>
</head>
<body>
    <img src="https://media.licdn.com/mediaC4D12AQEYSq4_V6tKhA" alt="MIT 6S.191 lecture on Deep Generative Models by Ava Soleimony 2022 Series of Deep Learning " title="MIT 6S.191 lecture on Deep Generative Models by Ava Soleimony 2022 Series of Deep Learning " />
      <h1><a href="https://www.linkedin.com/pulse/what-do-we-mean-latent-variables-ajay-taneja">Deep Generative Models </a></h1>
    <p class="created">Created on 2021-10-26 18:29</p>
  <p class="published">Published on 2022-04-05 21:35</p>
  <div><p><br></p><h2>1. Deep Generative Models: Latent Variable Models</h2><p>Autoencoders and Generative Adversarial Networks (GANs) are “Latent Variable Models” – one may ask at this point – what is a “Latent Variable” as the explanation might at times appear convoluted. I have found the analogy of the “Latent Variable” to work of Plato[<a href="https://lnkd.in/dht-MD-p" target="_blank">https://lnkd.in/dht-MD-p</a>] – Plato’s Cave – image shown below very insightful, thanks to MIT 6S.191 lecture on GANs by Ava Soleimany.</p><p>In Plato’s allegory, a group of prisoners face the wall as a punishment and there are some physical objects behind them which the prisoners cannot see – the prisoners can only see the shadows of these objects on the wall. The shadows are otherwise the “observations” which the prisoners make – the observed variables. The physical objects are the “latent variables” the underlying variables governing the actual behaviour which we cannot directly see in Plato’s cave example.Analogous to Plato’s Cave, in GANs our aim is to find the underlying variables – the “latent variables" using only the observed data. </p><p>For example, a dataset may comprise of several millions of facial images of different people for a face detection model that has to be built.You might have to find out the exact distribution of faces with respect to different features like skin colour, hair colour, eyes colour, etc in order to ensure that there is an appropriate distribution of these features in the dataset – these features are not directly visible (like in Plato’s analogy the physical objects behind the prisoners) – these features are the “latent variables” in the problem.</p><p>Another important practical application of GANs is detection of outliers/ strange events which are not v well represented in the data like an unexpected sight of pedestrian suddenly crossing a highway/outlier detection – these physical objects again form the “latent variables”</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQGVdClV0QTewQ" src="https://media.licdn.com/dms/image/C4D12AQGVdClV0QTewQ/article-inline_image-shrink_1500_2232/0/1649194390393?e=1691625600&amp;v=beta&amp;t=xU9RfDOuf0-Nj0LnRj1mnjVR2sUfyMUH4HitOTDkvfM"></div><p>Figure: Deep Generative Models - Analogy to understand what is emant by "Latent Variables" | Examples of Deep Generative Models</p><h2>2. Autoencoders</h2><p><strong>Autoencoders</strong></p><p>In the above paragraphs , I have highlighted that Autoencoders, and Generative Adversarial Networks (GANs) are Latent Variable Models. The post then went into the details explaining the underlying significance of the terminology “Latent Variables” using the analogy with Plato’s Cave.</p><p>Let us now understand more on “Autoencoders” and highlight a bit more succinctly on how Deep Generative Models can be useful in practical scenarios</p><p><strong>Deep Generative Models: Unsupervised Learning Models</strong></p><p>Autoencoders are essentially Unsupervised Learning models wherein we just have the Data but no labels or a numerical output. In Unsupervised Learning problems we would want to learn the underlying structure of the raw data. Some examples of unsupervised learning problems include clustering, dimensionality reduction and deep generative models – which are a topic of the series of these posts</p><p><strong>Autoencoders: Background and how they work? </strong></p><p>The main idea of the Autoencoders is to learn some “encoding” of the input and then “decode” and reconstruct the input. The reasons behind undergoing this exercise and the applications are discussed in the subsequent paragraphs. &nbsp;</p><p>Autoencoders take the input of the raw data – as shown in the figure below and pass it through the series of the layers in the deep neural network and then get a lower dimension latent space (or, lower dimension feature space) z as in the figure. One may now ask, why is it necessary to go from a higher dimension feature space to a lower dimension feature space? The answer is that going from a higher dimension feature space to a lower dimension feature space we’re in fact compressing the feature space / compressing it and giving a meaningful representation of the input data. </p><p>&nbsp;</p><p><strong>How do you train the model as there are no “labels” or any numerical output?</strong></p><p>As mentioned above, Autoencoders are unsupervised learning models, so we do not have the labels/output in the training data. So, how do we test such networks – because we do not have any labels/output, we decode the network (after encoding it into a lower dimension feature space) so that we obtain the compressed data we encode it back to get the original data – that is, we get the reconstructed output – in the figure this is termed as x_bar. Things become much simpler now as we can train the network by simply estimating mean square error represented by L(x,x_bar) = ||x-x_bar||^2 which denotes the mean square error between the original input and the reconstructed output.&nbsp;</p><p>The encoded lower diemension feature space is much more "compact" than the original input and highlights a lot more information of the data in scenarios such as: a) detecting the spread of feature space covering different features such as hair colour, skin colour, etc in face detection model comprising of huge datasets b) detecting outliers from huge datasets for building autonomous vehicle model, etc</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C5612AQE3IK5PJl8fsA" src="https://media.licdn.com/dms/image/C5612AQE3IK5PJl8fsA/article-inline_image-shrink_400_744/0/1649359213362?e=1691625600&amp;v=beta&amp;t=eBVpxfQF7oHu9z6SIGYdZC_-mzGjQ33JVbq7gKUnz84"></div><p><strong>                     Figure: Autoencoders - Encoding - Decoding Network</strong></p><p><br></p><p>This post builds a bit more and gets into the “Variational Autoencoders” – commonly referred as VACs which are very commonly used in Generative Models.</p><h2><br></h2><h2>3. Variational Autoencoders</h2><p>Let us now build up a bit more and gets into the “Variational Autoencoders” – commonly referred as VACs which are very commonly used in Generative Models.</p><p>&nbsp;<strong>Traditional Autoencoders vs Variational Autoencoders :</strong></p><p>In traditional Autoencoders, we take the input x, “encode” it to get a more compact feature space z and then we decode it again to get back the input now denoted by – z_bar as shown in the figure below.&nbsp;</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C5612AQGrW-Rn7BmcdA" src="https://media.licdn.com/dms/image/C5612AQGrW-Rn7BmcdA/article-inline_image-shrink_1000_1488/0/1649691896280?e=1691625600&amp;v=beta&amp;t=ECgcTJgWN7_hU0vNUkGHYEp6uXRedkQ_SxHgh6VnsB4"></div><p>Thus, in Autoencoders, we always get back the <strong>same input</strong>. Thus, in Autoencoders, the input and the output we are learning is deterministic – that is, we always get the exact match. However, in case of Variational Autoencoders (VAEs), we would like to generate new images/data that we were not able to generate using traditional autoencoders as it was deterministic. That is: using traditional autoencoders we were not able to generate new synthetic data that would make our original data set “richer”.&nbsp;</p><p><strong>What do Variational Autoencoders (VAEs) do?</strong></p><p>Using Variational Autoencoders, we attempt to add some randomness / probability/stochasticity to the data so that we generate new synthetic data (or, new images) from the model. Using the element of probability, we also learn a smoother representation of the latent space which is not always possible using traditional Autoencoders.</p><p>Using VACs, we break down the latent space into a mean and a standard deviation vector for the latent variables z as shown in the figure. This element of randomness will help generate synthetic data and build up more informative latent space as well.</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C5612AQFRX6sI9YG4FQ" src="https://media.licdn.com/dms/image/C5612AQFRX6sI9YG4FQ/article-inline_image-shrink_1000_1488/0/1649691963373?e=1691625600&amp;v=beta&amp;t=la0gy3qrCzH5CO5IpRJ9s_aYIH6VnWIo0DEZ-cDzoPw"></div><h2><strong>4. Synopsis on the notes above&nbsp;(I'm compiling this on another day and trying to charge my brains !):</strong></h2><p>The above paragraphs involving “Deep Generative Models” went into the details of the concept of Latent Variables” and into the conceptual working of Autoencoders and Variational Autoencoders (VAEs). Autoencoders involve 2 phases of learning process: “encoding” and “decoding” phase – during the encoding phase, we learn a compressed version of the input and this compact version of the input data helps in identifying latent variables which were not directly visible in the original input. In the decoding we reconstruct the input data again through layers of a deep neural network and this reconstruction aids in computing the loss and forms the underlying principle of how the overall network learns in Autoencoders.</p><p>In Variational Autoencoders VAEs) we add some stochasticity by associating the latent variables with a mean and standard deviation which helps in many ways such as: resulting in a more smoother representation of latent space, helps in causing some perturbances to the latent variables and seeing the effect of these perturbances on the overall latent space and most importantly aids in generating new data/new images.</p><p>Let us now get an insight of the weights in VAEs and Loss terms in VAEs which forms the subject of this post</p><p><br></p><h2><strong>5. Weights in Variational Autoencoders&nbsp;:</strong></h2><p>In Variational Autoencoders – as we have introduced an element of probability – both the encoding and decoding architecture are going to be probabilistic in nature. That is, during the course of the training, the encoder will try to infer a probability distribution of z given the input x and the decoder will try to infer a probability distribution of x_bar given z (Figure below). When we train the network, we learn 2 separate set of weights – one for encoder which we denote as Φ and the other for decoder which we denote as θ.</p><p><br></p><h2><strong>6. Loss Function in Variational Autoencoders (VAEs):</strong></h2><p>So – the loss function in Variational Autoencoders is going to be a function of the weights Φ and θ. The loss function is a sum of 2 terms: the term denoting the reconstruction and the term denoting the regularization. Thus, the loss is no longer comprised of just the reconstruction term as in traditional autoencoders but also the regularization term. As in other Deep Neural Networks, we’re going to optimize the loss with respect to the weights and the weights are going to be updated during the course of the training of the network.</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C5612AQHU2DNKfaOXGw" src="https://media.licdn.com/dms/image/C5612AQHU2DNKfaOXGw/article-inline_image-shrink_1500_2232/0/1649851326547?e=1691625600&amp;v=beta&amp;t=328AeVZyAAaZBIhQl0gNtOQWKLez3Tio66UAlHfhUA0"></div><p><strong>                                             Figure: VAE Optimisation </strong></p><p><strong>What are these Losses?</strong></p><p>The loss function in Variational Autoencoders is comprised of: Reconstruction and the Regularization loss as illustrated below:</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C5612AQHhB7NdRvSYoQ" src="https://media.licdn.com/dms/image/C5612AQHhB7NdRvSYoQ/article-inline_image-shrink_400_744/0/1649851418521?e=1691625600&amp;v=beta&amp;t=LoSbWtDzxKHNguyJ4H0O3N-D-XTryjtqKNULf8Uz56k"></div><p>The Reconstruction Loss is the same as that discussed in Autoencoders which may be computed as mean square error loss as discussed in section above </p><p>With respect to Variation Autoencoders, the additional term in the computation of losses is the Regularization Loss. The Regularization loss is denoted by the expression below:</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C5612AQGN9k-BhCi9mg" src="https://media.licdn.com/dms/image/C5612AQGN9k-BhCi9mg/article-inline_image-shrink_400_744/0/1649851471986?e=1691625600&amp;v=beta&amp;t=7Txik-HZ7hIELy-FPnRROVYvNkFWmBiJ9G0p8g2PZPw"></div><p><br></p><p>In the above expression, q_Φ (z | x) denotes the probability distribution of z given x. This is the distribution that the encoder is trying to learn. It is the distribution of the latent space z given the input data x. In order to regularize we place a “prior” on the latent distribution. The prior means an initial hypothesis of what the latent variable “z” could look like. This helps the network to enforce some structure based on the prior. </p><p>The common choice of the “prior” is a centred normal distribution with a mean of 0 and standard deviation of 1. The prior encourages the latent variables to be centred around the latent space and distribute the encoding smoothly. This is illustrated in the figure below:</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C5612AQEehXC2bxwQNQ" src="https://media.licdn.com/dms/image/C5612AQEehXC2bxwQNQ/article-inline_image-shrink_1000_1488/0/1649851540371?e=1691625600&amp;v=beta&amp;t=Jj_fPP_N57TugXfCXxMxbJfz1sayHB5IlprypM8S9Wk"></div><h2>7. Synopsis on the notes above&nbsp;(again, I'm compiling this on another day and trying to recharge my brains !):</h2><p>So far, in Posts 1 through 4, it has been underscored that Autoencoders and Variational Autoencoders are latent variable models wherein the network attempts to learn the actual / physical variables associated with a dataset using a set of observed variables. An analogy to Plato’s Cave often helps in an intuitive understanding of the concept of “latent variables”. </p><p>Autoencoders prove very useful in use-cases such as: s</p><ul><li>Say – you’re given a huge dataset for making a face detection model and you have to first ascertain the distribution of various features such as hair, skin colour – then, the compact latent space – z – helps in inferring this</li><li>&nbsp;Another, important use case of Autoencoders could be in case of autonomous driving applications. Let us we’re given a huge dataset to aid in model generation for autonomous driving the compact latent space z can help in detecting outliers/unexpected events as part of the data set in the latent space z.</li></ul><p>&nbsp;Going to the VAEs, it was emphasized that VAEs add an element of probability by associating the latent variables with a mean vector and a standard deviation vector which helps in generating “new” data. The model with VAEs is associated with 2 sets of weights to infer the probability of z given x – z being the latent space and another to infer the probability of x_bar given z, x_bar denotes the reconstructed data with am element of stochasticity. The loss in VAEs is comprised of terms:</p><ul><li>Reconstruction term</li><li>Regularisation term</li></ul><p>The Reconstruction term is same as Autoencoders, and the regularization term enforces a distribution (normal distribution) on the latent variables z with a mean of 0 and standard deviation of 1. Let us now see how the regularization process in VAEs helps in the overall learning process.</p><h2><strong>8. Intuition on regularization and the Normal prior:</strong></h2><p>One may now ask – what are the properties that you would want to achieve by Regularization:</p><ul><li>Continuity</li><li>Completeness</li></ul><p>The first property is: Continuity – that is: if 2 points are close in the latent space, then, they should remain so after decoding from the latent space</p><p>The next property is Completeness – that is: if we do some sampling from the latent space, we should get meaningful content after decoding</p><p>&nbsp;This may be intuitively observed through the figure below:</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C5612AQHz77AzDxOWWA" src="https://media.licdn.com/dms/image/C5612AQHz77AzDxOWWA/article-inline_image-shrink_1000_1488/0/1650138112397?e=1691625600&amp;v=beta&amp;t=hwPphX7TKS77-_ntAPBpSvPB-1PE_zDcHX5pMp-CKzs"></div><p><br></p><h2>9. Important highlight of Variational Autoencoders: Sampling from the Latent Space and perturbing values of individual latent variables</h2><p>Another highlight of Variational Autoencoders is that: because of notion of probability, we can sample from the latent space and perturb the values of individual latent variables keeping all other variables constant and generate data samples that are perturbed with a single feature. </p><p>An example is shown below where one latent variable (head pose) is changed in the reconstruction output.</p><p>&nbsp;The kind of generation of synthetic data might be useful in use cases such as synthetic data generation of autonomous driving and medical imaging.&nbsp;</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C5612AQEVwIwzx7Gd6Q" src="https://media.licdn.com/dms/image/C5612AQEVwIwzx7Gd6Q/article-inline_image-shrink_1000_1488/0/1650314095259?e=1691625600&amp;v=beta&amp;t=2ou6tFHYrVwa2uMqMkUVILluIEOVq5Ts4K_gVvhbfSc"></div><h2>10. <strong>Hyperparameters in the Loss Function</strong></h2><p>As mentioned in the above paragraphs, the loss function in Variational Autoencoders is comprised of reconstruction term and regularization term. The strength of the regularization term is controlled by hyperparameter β. If β &gt; 1 the encoding will encourage disentanglement i.e., more latent variables are subjected to disentanglement /perturbation.</p><p>Another use case of Deep Generative models is to make fair/unbiased datasets. Using certain distribution of latent features one can adjust / refine the dataset during training that can create a more representative dataset resulting in a more unbiased model</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C5612AQETnZd0FXMw-g" src="https://media.licdn.com/dms/image/C5612AQETnZd0FXMw-g/article-inline_image-shrink_1000_1488/0/1650314165384?e=1691625600&amp;v=beta&amp;t=8duTIMSWZHfP5lbWQQFNzgpc6rpTuwKNhJsr0Rg1tjQ"></div><p><br></p><p><br></p><h2><strong>11. Summary of Variational Autoencoders</strong></h2><p>From the paragraphs above, a summarized description of Variational Autoencoders is highlighted below:</p><ul><li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Autoencoders use a compressed representation of the input data to extract features which might not directly be visible in the dataset</li><li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Once the input data is encoded, reconstruction allows the loss to be computed, thus, the learning is “unsupervised”</li><li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;One can interpret the “hidden” latent variables using “perturbation function”</li><li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;One can sample from the latent space to generate “new” example datasets</li></ul><h2><strong>12. Generative Adversarial Networks (GANs)</strong></h2><p>Having discussed: Autoencoders, Variational Autoencoders (VAEs) and the applications of these “latent variable” models, let us now focus our attention on another type of generative model knows as – Generative Adversarial Networks (GANs) which aim to produce “new” / high quality images/data. It should be highlighted that the problem with VAEs is the estimation of the probability distribution of the latent variables and GANs overcome this difficulty as described below.</p><p><strong><u>How are GANs different from the VAEs?</u></strong></p><p>In GANs, we do not explicitly model the probability distribution of the data but instead use the distribution implicitly and generate the new instances of data. But since the input data is significantly complex, it is very difficult to generate new realistic samples directly. </p><p>The idea behind GANs is to start from something extremely simple, like random noise and use the power of neural networks to transform from something as simple as “random noise” to “new”/high quality data. This is the key breakthrough of the idea behind “GANs”. </p><p><br></p><p><strong><u>How do GANs transform random noise to “new” synthetic data:</u></strong></p><p>The way the GANs transform random noise to new synthetic data is by having 2 networks that in fact are adversarial (i.e., conflicting one another) in nature. GANs achieve this by having 2 neural networks in the overall architecture. These neural networks are termed as:</p><p>a)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Generator</p><p>b)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Discriminator</p><p>Generator turns the random noise into an imitation of the data and the Discriminator network receives the input from the Generator along with the “Real” and classifies the input as “fake” or “real”. This to-and-fro communication between the Generator and the Discriminator continues unto the Generator has transformed the noise into new synthetic data and the Discriminator is now not able to between the actual data and the data generated by the Generator.</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQHLQiFav5aooQ" src="https://media.licdn.com/dms/image/C4E12AQHLQiFav5aooQ/article-inline_image-shrink_1000_1488/0/1650654232742?e=1691625600&amp;v=beta&amp;t=T5eZ41FLQW--_05JIR4twoc6KTch8lveEzudC4rPjZ8"></div><h2><br></h2><h2><strong>13. Intuition on how the Generator and Discriminator operate</strong></h2><p>The operation / interaction between the Generator can be intuitively described as follows:</p><p>1)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The process begins by feeding the generator with some “fake” data which essentially is some random noise to start</p><p><br></p><p>2)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The Discriminator will now be fed with this fake data together will the real data and its task will be to output a probability that the data it sees is real or fake</p><p><br></p><p>3)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In the course of the training, the “Discriminator” will learn to correctly distinguish between the real and the fake data</p><p><br></p><p>4)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;There will be a stage reached when the Discriminator will be able to correctly distinguish between the real and the fake data and then we go back to the Generator wherein the Generator will see some examples of real data and will try to move the move the fake examples close to the real data</p><p><br></p><p>5)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The Discriminator will again estimate the probability and the Generator in turn will try to move the fake data close to the real data</p><p><br></p><p>6)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;At the end, the Generator will have brought the quality of the fake data close to that of the fake data close to that of the real data such that the fake examples created are identical to the real examples.</p><p><br></p><p>7)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Thus, the Generator at the end will be able to synthesize new data</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQEb91SQVK-BGQ" src="https://media.licdn.com/dms/image/C4E12AQEb91SQVK-BGQ/article-inline_image-shrink_400_744/0/1650654289379?e=1691625600&amp;v=beta&amp;t=1yZP8wMhCEvGeNw5vgCR8K9HLRG3hv9gAkyzcJHblew"></div><p><br></p><h2><strong>14. Loss functions in Generative Adversarial Networks</strong></h2><p><br></p><p><strong>Training GANs:</strong></p><p>The way we train GANs is by formulating an objective – a loss function that is known as “adversarial objective”. Our goal is to let the Generator to exactly reproduce the data distribution – however, in reality it is highly difficult to achieve the global optimum.<strong>Loss function: Objective of the Discriminator network</strong></p><p>The loss function is comprised of objectives for the Discriminator network and of objectives for the Generator network. Considering first the objectives for the Discriminator network – in the Discriminator network, we’re maximizing the probability of identifying the fake data ass “fake” and the real data as “real”</p><p><br></p><p>The loss function is a cross entropy loss (<a href="https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html" target="_blank">https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html</a>) between the true distribution and the distribution generated by the network.</p><p><br></p><p><strong>Loss function: Objective of the Generator network</strong></p><p><strong>&nbsp;</strong></p><p>The objective of the Generator network is to minimize the probability that the generated data identified is “fake” – this is because the Generator is attempting to match the fake data close to the Real data as explained in the paragraphs on “Intuition”</p><p><br></p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQG6swU0Emagug" src="https://media.licdn.com/dms/image/C4E12AQG6swU0Emagug/article-inline_image-shrink_1000_1488/0/1650654392211?e=1691625600&amp;v=beta&amp;t=uAVurCOyBPWmLVeCrHkEOSEcGgeQjZ8vbzpoFNkqIz4"></div><p><br></p><h2><strong>15. How to use the Generator once it has been trained?</strong></h2><p>Once we have trained the overall network, our goal is to use the Generator network and sample from it to create new data instances that have never been seen before. The Generator synthesizes the new data instances by going from a distribution of completely random <strong>Gaussian</strong> noise to map a function / transformation towards a target data distribution. One point in the noise distribution is going to lead to one point in the target distribution.</p><p><br></p><p>We can summarize as follows:</p><p>1)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We provide a generator some random noise. </p><p>2)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;It maps to some area of an image (as its trained already) - that area could be any pattern (forehead/cheeks/eyes/nose and so on) which it has learnt to map from the random noise </p><p>3)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Thereby, the generator can map to several patterns from different instances of random noise so as to finally get a completely different collection of patterns/ new image</p><p><br></p><p>Credits | References:</p><p>📁MIT6S.191 Deep Learning Series of 2022</p><p>📁GANs Specialization on Coursera</p><p>📁LinkedIn conversations with AI/ML/DL enthusiasts</p></div>
</body>
</html>