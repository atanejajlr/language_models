<html>
<head>
  <title>Foundational Principles of Deep Learning – my notes</title>
  <style>
    body {
      margin: 0 auto;
      width: 744px;
      font-family: Source Serif Pro, serif;
      line-height: 32px;
      font-weight: 400;
      color: rgba(0, 0, 0, 0.7);
      font-size: 21px;
    }
    h1, h2, h3 {
      font-family: Source Sans Pro, Helvetica, Arial, sans-serif;
    }
    h1 a, h1 a:visited {
      color: inherit;
      text-decoration: none;
    }
    h1 {
      line-height: 48px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 42px;
      margin: 32px 0 20px;
    }
    h2 {
      line-height: 32px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 26px;
      margin: 28px 0;
    }
    h3 {
      line-height: 28px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 21px;
      margin: 24px 0;
    }
    p {
      margin: 32px 0;
    }
    .created, .published {
      color: rgba(0, 0, 0, 0.55);
      font-size: 15px;
      line-height: 15px;
      margin: 20px 0;
    }
    .created + .published {
      margin-top: -12px;
    }
    blockquote {
      font-family: Georgia, Source Serif Pro, serif;
      font-style: italic;
      font-size: 24px;
      line-height: 36px;
      margin: 48px 120px;
      text-align: center;
    }
    a {
      word-wrap: break-word;
      outline: none;
      text-decoration: none;
      background-color: transparent;
      border: 0;
      color: #008CC9;
    }
    a:hover {
      text-decoration: underline;
    }
    a:visited {
      color: #8C68CB;
    }
    .center {
      text-align: center;
    }
    iframe {
      display: block;
      margin: 44px auto;
    }
    *:not(pre) + pre, pre:first-of-type {
      margin-top: 32px;
      padding-top: 32px;
    }
    pre:only-of-type {
      margin: 32px 0;
      padding: 32px;
    }
    pre {
      background: #F3F6F8;
      overflow-x: auto;
      display: block;
      font-size: 13px;
      font-family: monospace;
      line-height: 13px;
      padding: 0 32px 32px;
      white-space: pre;
    }
    a.embedded {
      background: #F3F6F8;
      display: block;
      padding: 32px;
      margin: 32px 0;
    }
    img {
      height: auto;
      max-width: 100%;
    }
    .slate-image-embed__resize-full-width img {
      width: 100%;
    }
    .series-logo {
      width: 48px;
      height: 48px;
      box-sizing: border-box;
      background-clip: content-box;
      border: 4px solid transparent;
      border-radius: 6px;
      object-fit: scale-down;
      float: left;
    }
    .series-title {
      font-size: 16px;
      font-weight: 600;
      vertical-align: top;
    }
    .series-description {
      color: rgba(0,0,0,.6);
      font-weight: 400;
      font-size: 14px;
      line-height: 20px;
    }
    div {
      margin: 32px 0;
    }
  </style>
</head>
<body>
    <img src="https://media.licdn.com/mediaD4E12AQHVzWOmdPNH1A" alt="" title="" />
      <h1><a href="https://www.linkedin.com/pulse/foundational-principles-deep-learning-my-notes-ajay-taneja">Foundational Principles of Deep Learning – my notes</a></h1>
    <p class="created">Created on 2023-05-06 17:44</p>
  <p class="published">Published on 2023-05-06 19:02</p>
  <div><h2>1.&nbsp;&nbsp;&nbsp;Introduction</h2><p>&nbsp;As I had mentioned some weeks ago, I will be publishing a series of blogs which will go into the finer details of the Transformer architecture. &nbsp;Each part of the series will involve theoretical insight as well as an insight from coding point of view into every unit/ concept involving Transformer architecture as illustrated below. Each respective part will go into the details of the following:</p><p><br></p><ul><li>Sentence Tokenization and Input Embedding</li><li>Positional Encoding</li><li>Layer Normalization</li><li>Self-Attention</li><li>Multi-Head Attention</li><li>Etc</li></ul><p>&nbsp;</p><p>However, before diving into the above ingredients, I felt that it is essential to document the following:</p><p>o&nbsp;&nbsp;Firstly – the foundational principles of Deep Learning</p><p>o&nbsp;&nbsp;Secondly – the evolution of Language Models</p><p>By revisiting the above 2 topics in (some) detail, the content related to the Transformer is better understood. Hence, I felt that it is definitely useful to spend time and effort into the above topics before diving into the working of Transformers (<em>because that's how, I've been evolving my learning journey until now!)</em>. </p><p>This article will go into the details of the foundational principles of Deep Learning and will cover the following aspects:</p><ul><li>Section 2 revisits the definitions of Artificial Intelligence, Machine Learning and Deep Learning</li><li>&nbsp;Section 3 describes the effectiveness/importance of Deep Learning </li><li>&nbsp;Section 4 revisits the concept a perceptron leading to the illustration of neural network.</li><li>Section 5 talks of the concept of perceptron, neural networks, </li><li>Section 6 goes into neural network training, cost function/objective function/empirical loss and optimizing the loss</li><li>Section 7 throws light on backpropagation which is a technique to compute the gradient using chain rule of differentiation</li><li>&nbsp;Regularization is discussed in section 8. </li></ul><p><br></p><p>Needless to mention, all the content discussed below is a collection of my notes from various courses that I’ve taken so far, YouTube Videos that I’ve watched and blog posts by other learners available as open source. This is documented for my own future reference, however, I’m obviously happy if it benefits anyone reading this content.</p><p><br></p><h2>2.&nbsp; Artificial Intelligence, Machine Learning and Deep Learning </h2><p>&nbsp;</p><p><strong>Artificial Intelligence:</strong></p><p>Talking of Intelligence, it means the processing of information such that we can use it to infer some future decision and action that we take. The field of Artificial Intelligence involves building of computer algorithms that do exactly the same thing: process information to infer some future decision.</p><p><strong>Machine Learning:</strong></p><p>Machine Learning is a subset of Artificial Intelligence that focusses specially on making a machine learn to the above based on some human/real experiences. Statistical techniques are used to enable machines to improve with experience.</p><p><strong>Deep Learning:</strong></p><p>Deep Learning is a subset of Machine Learning that uses multi-layered neural networks to extract patterns that occur within the data so that the network learns to perform the tasks which otherwise will require human intelligence.</p><p>This article will go into the foundational aspects of Deep Learning which focusses on making the computer learn different tasks directly from the raw data.</p><figure class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4E12AQG1Ec8khQMnfw" src="https://media.licdn.com/dms/image/D4E12AQG1Ec8khQMnfw/article-inline_image-shrink_400_744/0/1683395403090?e=1691625600&amp;v=beta&amp;t=uq67wqC8QgQMvCRzAGyocPl8gx5A2o8NsAbFkCSPsuw"><figcaption>Figure: Illustration of Artificial Intelligence, Machine Learning and Deep Learning</figcaption></figure><p><br></p><h2>3.&nbsp;What does Deep Learning have to offer?&nbsp;</h2><p>Traditional Machine Learning algorithms typically define features (patterns) in the data and typically a human with expert domain knowledge will uncover these features – the key idea of Deep Learning is that instead of a human defining these features, the machine extracts patterns in the data so that it can use those to make some decisions.</p><p>For example, for a face detection algorithm, a deep neural network will learn that in order to detect the face, it first detects the line, edges which can be combined to get mid-level features like corners and curves which in turn can be combined to&nbsp;in deeper layers of the neural network to form high level features like eyes, ears, node, etc. and then all these together will be able to detect&nbsp;the face.</p><p>All the learning is hierarchical starting from lower layers of the network as illustrated in the figure below.</p><figure class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4E12AQEOL5mcph2ckA" src="https://media.licdn.com/dms/image/D4E12AQEOL5mcph2ckA/article-inline_image-shrink_400_744/0/1683395545498?e=1691625600&amp;v=beta&amp;t=PfQVhxkQo6hZaaYTxXQfso7ngm0VMse_kPoYfS3XQM4"><figcaption>Figure: Hierarchical learning of the features in deep neural networks</figcaption></figure><h2><br></h2><h2>4.&nbsp;&nbsp;The Building Block of Deep Learning: The Perceptron </h2><p>&nbsp;</p><p>Let us now start with the fundamental building block of every single neural network that one may develop – which is a single neuron. In the deep learning language, a single neuron is called a perceptron. The perceptron is a single neuron, and its internal state is represented by a set of inputs x1 to xn which are multiplied by the corresponding weights and added together – we also add a bias term indicated as w0 as shown in the figure below.&nbsp;</p><figure class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4E12AQFnyUcm3NZRyw" src="https://media.licdn.com/dms/image/D4E12AQFnyUcm3NZRyw/article-inline_image-shrink_400_744/0/1683395672168?e=1691625600&amp;v=beta&amp;t=TCv7UnC6XYIyQ2onssRjAue0Uq0vrD4VYZ4aOWzSD1c"><figcaption>Internal state of a single neuron (perceptron)</figcaption></figure><p><br></p><p>Then, we take the single number after the addition and pass it through a non-linear activation function and that produces the final output of the perceptron which may be termed as y_bar as shown below.</p><figure class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4E12AQE_klIF4114-Q" src="https://media.licdn.com/dms/image/D4E12AQE_klIF4114-Q/article-inline_image-shrink_400_744/0/1683395926082?e=1691625600&amp;v=beta&amp;t=HWr23UIJfWALDL0C6-dsW1Cly5Rnk-JN1gNVah4_6O8"><figcaption>Final output of the perceptron</figcaption></figure><p><br></p><p>The process is mathematically represented by the equation below:</p><figure class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4E12AQEmGvhpUGM_2Q" src="https://media.licdn.com/dms/image/D4E12AQEmGvhpUGM_2Q/article-inline_image-shrink_400_744/0/1683396003161?e=1691625600&amp;v=beta&amp;t=w0ZHR8MG5kjx_hK7pRIjzHB4Ey9r7cG6UAlfRIf4JcA"></figure><figure class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4E12AQGVKn9G6GKsMQ" src="https://media.licdn.com/dms/image/D4E12AQGVKn9G6GKsMQ/article-inline_image-shrink_400_744/0/1683396012169?e=1691625600&amp;v=beta&amp;t=L9e-caprkb2JVDwF01KRNn4raKFm1EcKri__lmP9lsY"><figcaption>Mathematical representation of the above process</figcaption></figure><p><strong>Purpose of activation function:</strong></p><p>The point of non-linear activation function is to introduce non-linearities to the data/ Almost all real-world data linear in nature, thus, if we want to deal with those datasets, we need models which are also non-linear so that the models can capture the kind of patterns in the data.</p><p>To understand this better, let us say we have a dataset as shown in the figure below:</p><figure class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4E12AQF8kTIdvT8l2g" src="https://media.licdn.com/dms/image/D4E12AQF8kTIdvT8l2g/article-inline_image-shrink_400_744/0/1683396172660?e=1691625600&amp;v=beta&amp;t=vyQ9M5SwKgakLRHx47OQ80eaRIyMKpps4Pn9YmsYFkk"><figcaption>A non-linear dataset</figcaption></figure><p>Suppose given this dataset, we have to construct a decision boundary i.e., a boundary separating the red and the green dots in the figure above. Now, if were to use only a straight line to separate the green and the red points, the best we could do is to separate as shown in the figure below:</p><figure class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4E12AQHxhwggfbWNdw" src="https://media.licdn.com/dms/image/D4E12AQHxhwggfbWNdw/article-inline_image-shrink_400_744/0/1683396206982?e=1691625600&amp;v=beta&amp;t=Lfze-81QdVaCdhXOQglVM9hT4uydbzQN1yGktwl53M0"><figcaption>Straight line (linear approach) to construct the decision boundary.</figcaption></figure><p>Thus, the problem cannot be solved effectively using a linear approach and we will have to resort to non-linearity which helps to deal with such types of problems. The non-linear activation functions allow us to deal with non-linear data which makes the neural networks very powerful.&nbsp;</p><figure class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4E12AQGrNQqrInXOkg" src="https://media.licdn.com/dms/image/D4E12AQGrNQqrInXOkg/article-inline_image-shrink_400_744/0/1683396255118?e=1691625600&amp;v=beta&amp;t=D9d01GulQgXWQLKyk_ZYvQ8PM_epsxvTzHAS5brH6Bs"><figcaption>Decision boundary after using on-linear activation functions</figcaption></figure><p>Further, it may be underscored that since we’re just multiplying the inputs with the corresponding weights and adding them together, the problem remains a linear problem until we introduce non-linearities using non-linear activation functions.</p><p><strong>Types of non-linear activation functions:</strong></p><p>Some types of activation functions include,</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sigmoid Activation Function</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Tan hyperbolic/ Hyperbolic Tangent activation function</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ReLu activation function</p><p>These are illustrated in the figure below:</p><figure class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4E12AQH0ejNyaCmH5Q" src="https://media.licdn.com/dms/image/D4E12AQH0ejNyaCmH5Q/article-inline_image-shrink_400_744/0/1683396318079?e=1691625600&amp;v=beta&amp;t=NIMEZHtzko4S1P-Y9aImTQGU8CIfYpOTKoy9WEql6A0"><figcaption>Activation Functions</figcaption></figure><h2><strong>5.&nbsp;&nbsp;</strong>Perceptron To Neural Network</h2><p>&nbsp;</p><p>Continuing with the above discussion: Now, let’s take the (single) perceptron and build something more striking!</p><p>Now, suppose we want 2 outputs from the function. We simply add another perceptron – this added / second perceptron will have its own sets of weights. Each perceptron controls the output of its associated piece.</p><figure class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4E12AQFE_0QTo7TcQw" src="https://media.licdn.com/dms/image/D4E12AQFE_0QTo7TcQw/article-inline_image-shrink_400_744/0/1683396600335?e=1691625600&amp;v=beta&amp;t=4VJGQUomF6-WlbwoxxjgzdjSVKCJ7_1M8IlMZHGCjps"><figcaption>Single layered neural network with 2 perceptron’s</figcaption></figure><p>&nbsp;Further, such perceptron’s can be stacked to form a single layered neural network as shown below:</p><figure class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4E12AQHI-fhxAPwdQw" src="https://media.licdn.com/dms/image/D4E12AQHI-fhxAPwdQw/article-inline_image-shrink_400_744/0/1683396664222?e=1691625600&amp;v=beta&amp;t=PTFUMuKyjQS6lMHRA9wG7Hv9XaF8E4iNEvcVJVw5r_Y"><figcaption>Single layered neural network</figcaption></figure><p>A deep neural network can be built by stacking more such sequential layers as shown in the figures below:</p><figure class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4E12AQG6jXRBeQnZJg" src="https://media.licdn.com/dms/image/D4E12AQG6jXRBeQnZJg/article-inline_image-shrink_400_744/0/1683396693522?e=1691625600&amp;v=beta&amp;t=wcBfhQhehzncdlNSYMIjVrAsn0aGngvmjAXSW7WmWpY"><figcaption>Deep Neural network with 3 hidden layers</figcaption></figure><p>With this illustration, we can imagine/interpret that the given inputs (at the beginning) are transformed into a new dimensional space with the values closer to what we want (i.e., closer to the output we want) and this transformation has to be learnt and this is described in the next section which pertains to the loss function (or objective function).</p><p><br></p><h2>6.&nbsp;Training a neural network</h2><p>&nbsp;</p><p><strong>Loss Function:</strong></p><p>Having constructed the neural network (single/multi layered) and if we just start utilizing the network – with random values of weights – to predict the output, the network will not predict correctly because it’s not yet been trained. The network does not have the information of the world concerning the problem!</p><p>To train the network, we will have to construct the <strong>loss function</strong> which will tell us how far the predicted output is from the actual output. The loss of the network measures the cost incurred from incorrect prediction. The loss function is also termed as the objective function or the cost function or empirical loss and is a measure of the total loss over the entire dataset. Mathematically, the loss function is expressed as follows:</p><figure class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4E12AQGLXyVR7eM4Sw" src="https://media.licdn.com/dms/image/D4E12AQGLXyVR7eM4Sw/article-inline_image-shrink_400_744/0/1683396838126?e=1691625600&amp;v=beta&amp;t=NDgH5wDMq_HT2hS3mqCWuujiKFWLKRIa1N1HMEEcpJ8"><figcaption>The Objective / Loss Function</figcaption></figure><p><br></p><p>As it may be noticed from the equation above, the loss function is a function of the inputs and the weights – i.e., the predicted output and the actual output.</p><p>&nbsp;</p><p><strong>Minimizing the Loss:</strong></p><p>Training the neural network will not only involve determining how far the predicted output is from the actual output but also <strong>minimizing the loss</strong>. Thus, mathematically we want to find the network weights that will result in the smallest loss as possible over the <strong>entire dataset</strong>. The mathematical equation is represented as follows:</p><figure class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4E12AQHAMSao8fUuvQ" src="https://media.licdn.com/dms/image/D4E12AQHAMSao8fUuvQ/article-inline_image-shrink_400_744/0/1683397058376?e=1691625600&amp;v=beta&amp;t=X67AuQ-xCY4nnyBl0b9Dty_64Dk_rdfBhH6BsBzjUMs"></figure><figure class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4E12AQGZ9YWDbgSRRQ" src="https://media.licdn.com/dms/image/D4E12AQGZ9YWDbgSRRQ/article-inline_image-shrink_400_744/0/1683397066072?e=1691625600&amp;v=beta&amp;t=gUfB_LVSsOhaGD50L8S6ykWowLcJp4dT-YGZodQ4A38"><figcaption>Mathematical illustration of minimization of the loss</figcaption></figure><p><br></p><p><strong>Cross entropy loss</strong></p><p>For a binary classification problem, the loss function employed is cross entropy loss denoted as below:</p><figure class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4E12AQEJp3u4jKOd0w" src="https://media.licdn.com/dms/image/D4E12AQEJp3u4jKOd0w/article-inline_image-shrink_400_744/0/1683397124253?e=1691625600&amp;v=beta&amp;t=3qyjNNT5Lkht9NXZMJU6jjT_XAbubl43YKS1rr2JYVU"><figcaption>Mathematical illustration of the cross entropy loss</figcaption></figure><p><br></p><p><strong>Mean squared error loss:</strong></p><p>Mean squared error loss can be used for regression models that can output continuous real numbers denoted as below:</p><figure class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4E12AQEAKyItO3og8A" src="https://media.licdn.com/dms/image/D4E12AQEAKyItO3og8A/article-inline_image-shrink_400_744/0/1683397236248?e=1691625600&amp;v=beta&amp;t=TB4Qchdk_XLAwD4tVvvE1UdxEjIZ7mBVfwGsFBVwW1k"><figcaption>Mathematical illustration of the mean squared error loss</figcaption></figure><p><br></p><p><strong>Loss optimization: How to minimize the loss?</strong></p><p>The loss function is going to be the function of the weights – for a 2-dimensional problem, this loss function can be visualized as follows:</p><figure class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4E12AQGnO2yL9YgIZQ" src="https://media.licdn.com/dms/image/D4E12AQGnO2yL9YgIZQ/article-inline_image-shrink_400_744/0/1683397902196?e=1691625600&amp;v=beta&amp;t=xjN0mtposiDFcJgO933GBIR1Litmq1X-13-ImundP1E"><figcaption>Variation of loss function for different values of the weights</figcaption></figure><p>In the above landscape, we want to find the least loss which will correspond to the lowest point. </p><p>This is done mathematically through the following steps:</p><ol><li>Firstly, we start at a random space and compute the loss at the specific location.</li><li>We then calculate how the loss is changing – i.e., we compute the gradient of the loss. The process of computing the gradient is known as “<strong>backpropagation</strong>”.</li><li>The gradient tells us how the loss is changing as a function of the weights.</li><li>&nbsp;We update the weights in a direction opposite to that of the gradient.</li><li>&nbsp;We continue the above process until we get to the lowest point.</li></ol><p>&nbsp;</p><p>The above algorithm is formally termed as gradient descent. Formally, the steps in the Gradient Descent algorithm may be highlighted as follows:</p><ul><li>Initialize the weights of the network randomly.</li><li>&nbsp;Loop until convergence the following:</li></ul><p>1) Compute gradient </p><p>2) Update weights in the direction opposite to that of the gradient</p><figure class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4E12AQGrIoGiBYzdBg" src="https://media.licdn.com/dms/image/D4E12AQGrIoGiBYzdBg/article-inline_image-shrink_400_744/0/1683398102243?e=1691625600&amp;v=beta&amp;t=B_WcVNqjbGCpNVJnQGLPgevdMMqIJC9EJavXt8ZWIYY"><figcaption>Mathematical representation of the gradientt and weight updates</figcaption></figure><p><br></p><p>The weights are updated in the direction opposite to that of the gradient. The parameter η is a small step that we take in the direction opposite to that of the gradient and is commonly termed as the “learning rate”. </p><p>3) Return the weights.</p><p><br></p><h2>7. Backpropagation:</h2><p>&nbsp;</p><p>The process of computing the gradient is termed as Backpropagation.</p><p>Mathematically, for a single layered neural network with two neurons as shown below, the gradient is computed using the chain rule of differentiation – backwards from the loss function throughout the output – as follows:</p><figure class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4E12AQF9T1aDK7uUmg" src="https://media.licdn.com/dms/image/D4E12AQF9T1aDK7uUmg/article-inline_image-shrink_400_744/0/1683398258474?e=1691625600&amp;v=beta&amp;t=Pp09T8aWJb82svk2T3YH00vQ0M7yVaCoxOD_65GpCyU"><figcaption>Diagramatic representation of moving abckwards from the loss function to compute gradient using the chain rule of differentiation (Backpropagation)</figcaption></figure><figure class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4E12AQHMO_8sN2AemA" src="https://media.licdn.com/dms/image/D4E12AQHMO_8sN2AemA/article-inline_image-shrink_400_744/0/1683398394861?e=1691625600&amp;v=beta&amp;t=ij-AFcpGqX90__HPoroU1-9T_DKDZGKgDUEkctPPMog"></figure><figure class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4E12AQEajO3qfjunpA" src="https://media.licdn.com/dms/image/D4E12AQEajO3qfjunpA/article-inline_image-shrink_400_744/0/1683398407208?e=1691625600&amp;v=beta&amp;t=dbOdwwjnkXiaJJ3P3EVbjKUe4e0sAmlVIv7-gVwdNtg"></figure><figure class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4E12AQFEOZ0P1A1MAQ" src="https://media.licdn.com/dms/image/D4E12AQFEOZ0P1A1MAQ/article-inline_image-shrink_400_744/0/1683398422213?e=1691625600&amp;v=beta&amp;t=FF27Lntg1TbtALdHEcznj6S31ceB_zvjT5outBw9jr8"><figcaption>Mathematics of gradient computation</figcaption></figure><p><br></p><p>The algorithm of backpropagation is decades old, and the paper (1986) can be found here: <a href="https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf" target="_blank">https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf</a></p><p><br></p><p>It must be underscored that the landscape of the cost function involved in a deep neural network is highly complicated than the one shown above!</p><p><br></p><figure class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4E12AQHyI7NQpMNffQ" src="https://media.licdn.com/dms/image/D4E12AQHyI7NQpMNffQ/article-inline_image-shrink_400_744/0/1683398582146?e=1691625600&amp;v=beta&amp;t=6CzTf72rhCaoLMsMXD44E4KXPW6FPzsOW6F963b-YN8"><figcaption>Loss function in a deep neural network</figcaption></figure><p><strong>Setting the learning rate: η</strong></p><p>Setting the learning rate can have very large consequences while building the neural network, Having the learning rate too small will make the travel to the lowest point in the landscape too slow (convergence is slow) whereas if the learning rate is high the calculation might bypass the point of global minima as intuitively illustrated below.</p><figure class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4E12AQGgsigvJpKQ2g" src="https://media.licdn.com/dms/image/D4E12AQGgsigvJpKQ2g/article-inline_image-shrink_400_744/0/1683398653130?e=1691625600&amp;v=beta&amp;t=oTS-zpsw04lxYJUp_Zr2JRSlfLBgWOsCv6JmmB-t1G8"><figcaption>Learning rates overshooting</figcaption></figure><p>In practise an adaptive process is followed wherein the “learning rate” “adapts” to the landscape. When we say “adapt”, it means that the learning rate can be made smaller or larger depending on:</p><ul><li>How large the gradient is</li><li>How fast the learning is happening.</li><li>Size of weights</li><li>Etc.</li></ul><p>More details can be found here: <a href="https://www.ruder.io/optimizing-gradient-descent/" target="_blank">https://www.ruder.io/optimizing-gradient-descent/</a></p><p><br></p><h2>8.&nbsp;Regularization:</h2><p>𝗗𝗿𝗼𝗽𝗼𝘂𝘁𝘀</p><p>In the case of Neural Networks, regularization is typically done using "Dropouts". In Dropouts, during training, we essentially select randomly some subset of neurons in the neural network and prune these neurons with some probability. We randomly turn these neurons on and off at different iterations during training.</p><p>This essentially forces the neural network to learn an "ensemble" of different models. 𝐼𝑡 𝑐𝑎𝑛 𝑏𝑒 𝑖𝑛𝑡𝑒𝑟𝑝𝑟𝑒𝑡𝑒𝑑 𝑠𝑜 𝑏𝑒𝑐𝑎𝑢𝑠𝑒 𝑎𝑡 𝑒𝑣𝑒𝑟𝑦 𝑖𝑡𝑒𝑟𝑎𝑡𝑖𝑜𝑛 𝑡ℎ𝑒 𝑛𝑒𝑡𝑤𝑜𝑟𝑘 𝑤𝑖𝑙𝑙 𝑏𝑒 𝑒𝑥𝑝𝑜𝑠𝑒𝑑 𝑡𝑜 𝑑𝑖𝑓𝑓𝑒𝑟𝑒𝑛𝑡 𝑚𝑜𝑑𝑒𝑙𝑠 𝑖𝑛𝑡𝑒𝑟𝑛𝑎𝑙𝑙𝑦 𝑡ℎ𝑎𝑛 𝑡ℎ𝑒 𝑜𝑛𝑒 𝑖𝑡 ℎ𝑎𝑑 𝑜𝑛 𝑡ℎ𝑒 𝑝𝑟𝑒𝑣𝑖𝑜𝑢𝑠 𝑖𝑡𝑒𝑟𝑎𝑡𝑖𝑜𝑛𝑠 𝑏𝑒𝑐𝑎𝑢𝑠𝑒 𝑎 𝑑𝑖𝑓𝑓𝑒𝑟𝑒𝑛𝑡 𝑠𝑒𝑡 𝑜𝑓 𝑛𝑒𝑢𝑟𝑜𝑛𝑠 𝑎𝑟𝑒 𝑡𝑢𝑟𝑛𝑒𝑑 𝑜𝑛 𝑎𝑛𝑑 𝑜𝑓𝑓. This results in being a very powerful technique and helps in generalizing better.</p><figure class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4E12AQEXmpSNjolGHw" src="https://media.licdn.com/dms/image/D4E12AQEXmpSNjolGHw/article-inline_image-shrink_400_744/0/1683398770005?e=1691625600&amp;v=beta&amp;t=mWe6vjS8GO-xvRIADDTR8niE4rSBvo3j_Y9KwKEaXsc"><figcaption>Dropouts – Pruning of neurons during iterations of training</figcaption></figure><p><br></p><p>𝗘𝗮𝗿𝗹𝘆 𝗦𝘁𝗼𝗽𝗽𝗶𝗻𝗴</p><p> </p><p>The next regularization technique often practised for Neural Networks in "Early Stopping". Here the Data Scientist will normally plot the performance of the network on the training and the test data. As the network is trained, one would notice both the training and the test set loss decrease but a stage is reached where the training error continues to decrease but the test set error begins to increase. IT is at this point essentially that the model is beginning to overfit. And it is this point one would want to stop the training process as otherwise the model will learn the training data very precisely but not perform well on unseen data (overfitting).</p><figure class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4E12AQG-iDgolUwW7Q" src="https://media.licdn.com/dms/image/D4E12AQG-iDgolUwW7Q/article-inline_image-shrink_400_744/0/1683398821471?e=1691625600&amp;v=beta&amp;t=FtWAQuooHlXEifzFrlPbxrOydwik9ZHWklbRqyXGKT0"><figcaption>Early stopping</figcaption></figure><p>I have discussed in (some) detail on Regularization in an earlier blog here: </p><p><a href="https://www.linkedin.com/pulse/understanding-regularization-ajay-taneja/?utm_source=share&amp;utm_medium=member_android&amp;utm_campaign=share_via" target="_blank">https://www.linkedin.com/pulse/understanding-regularization-ajay-taneja/?utm_source=share&amp;utm_medium=member_android&amp;utm_campaign=share_via</a></p></div>
</body>
</html>