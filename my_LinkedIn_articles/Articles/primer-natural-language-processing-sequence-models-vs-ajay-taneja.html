<html>
<head>
  <title>A Primer on Natural Language Processing: Sequence models vs. Attention models</title>
  <style>
    body {
      margin: 0 auto;
      width: 744px;
      font-family: Source Serif Pro, serif;
      line-height: 32px;
      font-weight: 400;
      color: rgba(0, 0, 0, 0.7);
      font-size: 21px;
    }
    h1, h2, h3 {
      font-family: Source Sans Pro, Helvetica, Arial, sans-serif;
    }
    h1 a, h1 a:visited {
      color: inherit;
      text-decoration: none;
    }
    h1 {
      line-height: 48px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 42px;
      margin: 32px 0 20px;
    }
    h2 {
      line-height: 32px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 26px;
      margin: 28px 0;
    }
    h3 {
      line-height: 28px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 21px;
      margin: 24px 0;
    }
    p {
      margin: 32px 0;
    }
    .created, .published {
      color: rgba(0, 0, 0, 0.55);
      font-size: 15px;
      line-height: 15px;
      margin: 20px 0;
    }
    .created + .published {
      margin-top: -12px;
    }
    blockquote {
      font-family: Georgia, Source Serif Pro, serif;
      font-style: italic;
      font-size: 24px;
      line-height: 36px;
      margin: 48px 120px;
      text-align: center;
    }
    a {
      word-wrap: break-word;
      outline: none;
      text-decoration: none;
      background-color: transparent;
      border: 0;
      color: #008CC9;
    }
    a:hover {
      text-decoration: underline;
    }
    a:visited {
      color: #8C68CB;
    }
    .center {
      text-align: center;
    }
    iframe {
      display: block;
      margin: 44px auto;
    }
    *:not(pre) + pre, pre:first-of-type {
      margin-top: 32px;
      padding-top: 32px;
    }
    pre:only-of-type {
      margin: 32px 0;
      padding: 32px;
    }
    pre {
      background: #F3F6F8;
      overflow-x: auto;
      display: block;
      font-size: 13px;
      font-family: monospace;
      line-height: 13px;
      padding: 0 32px 32px;
      white-space: pre;
    }
    a.embedded {
      background: #F3F6F8;
      display: block;
      padding: 32px;
      margin: 32px 0;
    }
    img {
      height: auto;
      max-width: 100%;
    }
    .slate-image-embed__resize-full-width img {
      width: 100%;
    }
    .series-logo {
      width: 48px;
      height: 48px;
      box-sizing: border-box;
      background-clip: content-box;
      border: 4px solid transparent;
      border-radius: 6px;
      object-fit: scale-down;
      float: left;
    }
    .series-title {
      font-size: 16px;
      font-weight: 600;
      vertical-align: top;
    }
    .series-description {
      color: rgba(0,0,0,.6);
      font-weight: 400;
      font-size: 14px;
      line-height: 20px;
    }
    div {
      margin: 32px 0;
    }
  </style>
</head>
<body>
    <img src="https://media.licdn.com/mediaC4D12AQEv-RNe8IQTIA" alt="" title="" />
      <h1><a href="https://www.linkedin.com/pulse/primer-natural-language-processing-sequence-models-vs-ajay-taneja">A Primer on Natural Language Processing: Sequence models vs. Attention models</a></h1>
    <p class="created">Created on 2021-02-27 20:24</p>
  <p class="published">Published on 2021-08-26 20:31</p>
  <div><p><strong><em>Abstract: </em></strong></p><p><em>This blog is a collection of my notes – consolidated/summarized&nbsp;from various sources, including: &nbsp;the coursera specialization on Natural Language Processing available at deeplearning.ai, the MIT course on Introduction to Deep Learning, research papers as highlighted through the sections below, several You-Tube videos and blog posts of AI enthusiasts and practitioners available as open source and my own interpretation on the subject of Natural Language Processing . I’ve emphasized mainly on Sequence Models and Attention Models in this blog post with some reference to Probabilistic Models in the sections below.</em></p><p><em>This blog post (I prefer to call it as a Primer!) is comprised of 15 sections. Through the introductory section I have attempted to intuitively explain the concept of Sequence Models. In the section 2, I have attempted to explain the difference between the feed-forward neural network architecture and the architecture required for sequence modelling problems.  Following this, the recurrent neural network are introduced and then elaborated from sections 3 through 7. The limitations of RNNs are discussed in section 8 and LSTMs (Long Short-Term Memory Units) are introduced and elaborated in section 9. Although LSTMs solve some of the problems faced in RNNs, they are extremely slow to train as well as fail many times to capture long term dependencies – as one would expect this leads to the "most elegant" of all "Transformer Models", in sections 11 through 14. The BERT (Bidirectional Encoding Representation of Transformers) is elaborated in section 15.</em></p><p><br></p><h2>1.&nbsp;&nbsp;&nbsp;Introduction:</h2><p><br></p><p>If one understands the essentials of neural networks – starting from the concept of perceptron to feed forward models, one could then build upon this understanding to apply neural networks to several problems which involve sequence processing of data. The kind of problems or tasks which involve sequence processing of data require different kind of network architecture than that used for solving a simplistic binary classification problems such as given the input of students in&nbsp;a class against the number of lectures attended through the year – we’re trying to determine of the student will pass the examination or not or a more involved (and interesting!) engineering regression problem&nbsp;wherein we have to develop a life estimation correlation considering effects of: plasticity, creep, area correction, surface roughness correction thus augmenting the value worst directional principal strain through these factors.</p><p>Firstly, let us try and motivate ourselves on why problems involving sequence modelling are to be treated differently through a simple intuitive example. Let's suppose we have this picture here of a ball and our task is to predict where it's going to travel to next.&nbsp;However, with this information only – i.e. the position of the ball at any given instant we essentially have no clue whatsoever of where the ball would travel next.</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQHl3yqvT-_gLQ" src="https://media.licdn.com/dms/image/C4D12AQHl3yqvT-_gLQ/article-inline_image-shrink_1000_1488/0/1630009577879?e=1691625600&amp;v=beta&amp;t=RfTpfmlp7UCBB6Hhkd5xOKKAJhp8xfWj2H5cva80_so"></div><p><strong>Figure: How do we predict the next position of the ball without any prior history?</strong></p><p><br></p><p>Thus, without any prior information about the ball’s history or understanding of the dynamics of its motion we just cannot guess the next position of the ball! But instead if in addition to the current location of the ball we also know its previous locations, then our problem becomes much easier and we develop some sense on where the ball is going to travel to next. Thus, this intuitive example gives us a sense of what we mean in terms of sequential modelling and sequential prediction&nbsp;</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQGTKe2qLXeMuw" src="https://media.licdn.com/dms/image/C4D12AQGTKe2qLXeMuw/article-inline_image-shrink_1000_1488/0/1630009670316?e=1691625600&amp;v=beta&amp;t=JQ9vOPOmS6Wb6nQNuyr-osato4fLGMiGtlp0fH7R3LE"></div><p><strong>Figure: Sequence modelling to predict the position of the ball at a subsequent instant</strong></p><p><br></p><p>And the truth is that sequence data and the type of problems are everywhere around us, for example,</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Audio like waveform from a speech can be split into sequence of sound waves</p><p><br></p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Text can be split into a sequence of characters or words and each of these individual characters or words can be thought of as a timestep or a sequence</p><p><br></p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;There are many more applications where sequence processing can be useful from, medical signals to reading of electrocardiograms or for predictions of stock prices</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQFKjgF2n5pH0g" src="https://media.licdn.com/dms/image/C4D12AQFKjgF2n5pH0g/article-inline_image-shrink_1000_1488/0/1629644916010?e=1691625600&amp;v=beta&amp;t=UI7SMrjRKFydfVmmnWnhNnxeJroORFBMSYRqQRplL4Y"></div><p><strong>                       Figure: Examples of sequence processing – Audio </strong></p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQHuVW2uTxzEOQ" src="https://media.licdn.com/dms/image/C4D12AQHuVW2uTxzEOQ/article-inline_image-shrink_1000_1488/0/1629644939662?e=1691625600&amp;v=beta&amp;t=kWg2JHqmNqZZUROqJL86PEa7Q1AtDsnJDuwnieNP-Ok"></div><p><strong>                          Figure: Examples of sequence processing – ECG Reading </strong></p><h2>2.&nbsp;&nbsp;&nbsp;Building Neural Networks to solve Sequence Modelling problems:</h2><p>Firstly, let us start from the very fundamentals by revisiting the concept of the perceptron and develop a solid understanding of the changes that should be done to the neural network architecture in order to handle sequential data. Starting from the concept of perceptron,&nbsp;we defined the set of inputs from&nbsp;x1 through xn&nbsp;and each of these numbers multiplied by a weight matrix&nbsp;and then they're going to all be added together&nbsp;to form this internal state of the perceptron&nbsp;&nbsp;which we'll say is z and then this value z is&nbsp;passed through a non-linear activation function&nbsp;to produce a predictive output y_hat as shown in the figure below;</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQEZvYU_Qj0sDw" src="https://media.licdn.com/dms/image/C4D12AQEZvYU_Qj0sDw/article-inline_image-shrink_1000_1488/0/1629644987843?e=1691625600&amp;v=beta&amp;t=mRaOL639201AlBOKPOOcIAXeMx0Gcn4mWV84QOXaZ3E"></div><p><strong>                                       Figure: The perceptron revisited</strong></p><p>It may be recalled that with the perceptron, one can have multiple inputs coming in and since we know we’re talking of sequence modelling, these inputs can be considered as being from a single time step in a sequence. We could extend from the single perceptron to now a layer of perceptron’s to yield&nbsp;&nbsp;multi-dimensional outputs as shown in the figure below;</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQHyOqfYS-HVxw" src="https://media.licdn.com/dms/image/C4D12AQHyOqfYS-HVxw/article-inline_image-shrink_1000_1488/0/1629645047432?e=1691625600&amp;v=beta&amp;t=ilrf7WiDfXGXxbU_4gKKdCnVfyh5GDOZQOdDodYnx20"></div><p><strong>                                        Figure: Feed forward neural network</strong></p><p>  It should be underscored that the above mechanism does not have a notion of time or a sequence. All the inputs and the outputs above can be thought of that coming from a fixed time step of a sequence. </p><p>Now – lets us simplify things. Let us simplify the diagram and collapse the hidden layer as shown:</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQEgFDPYZ9GY3A" src="https://media.licdn.com/dms/image/C4D12AQEgFDPYZ9GY3A/article-inline_image-shrink_1000_1488/0/1629645213791?e=1691625600&amp;v=beta&amp;t=uhnoU4d5Ly4sXeyzQ-ABk942NwlFZPDYhhLblsNQUkg"></div><p><br></p><p><br></p><p>Here, the input and the output vector are depicted of being of length m and length n respectively. Now, if we apply the same model repeatedly for each time step in the sequence, we get a sense of how we could handle the individual inputs across different time steps as shown in the figure below;</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQH4tTeVX6ZHpA" src="https://media.licdn.com/dms/image/C4D12AQH4tTeVX6ZHpA/article-inline_image-shrink_1000_1488/0/1629645242759?e=1691625600&amp;v=beta&amp;t=vbRIay_4PpXQAPSoIO1BN3d8duGRicFSQmuSfb9WNz0"></div><p><strong>                                        Figure: Handling individual time steps</strong></p><p>  <strong>&nbsp;</strong>All the models depicted above are replicas of each other at different time steps. The output vector y_hat_t is going to be the function of the input at that time step as shown below;</p><div class="slate-resizable-image-embed slate-image-embed__resize-middle"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQHTHPrLWhP65Q" src="https://media.licdn.com/dms/image/C4D12AQHTHPrLWhP65Q/article-inline_image-shrink_400_744/0/1629645400827?e=1691625600&amp;v=beta&amp;t=h-9mBq6aRFCVtC-EZLU5x0t6o-Md4o2sT_OVn8K9Wqk"></div><p>Now taking a step back we know if we’re considering sequence data it is very likely that considering the sequence data the output of a label at a&nbsp;particular time step is going to depend on inputs at the prior time steps – so, we cannot treat the individual time steps as isolated steps.&nbsp;Thus, we have to consider the relationship which is inherent to the sequence data considering the inputs between different time steps. So how to address this?</p><p><br></p><p>We need to link the information in the computation of the network at different time steps to each other. Specifically, we are going to introduce internal memory or cell state denoted h_t;</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQERm0fNbUqxjg" src="https://media.licdn.com/dms/image/C4D12AQERm0fNbUqxjg/article-inline_image-shrink_1000_1488/0/1629645483186?e=1691625600&amp;v=beta&amp;t=X357O5sdAU5NNZMwA7wRNgbzV0TY4cv3Q18AzWqZnRw"></div><p><br></p><p>The h_t is going to be the memory that will be maintained by the neurons and the network itself and this state can be passed from time step to time step across time. The key idea here is that by having the recurrence relation we’re capturing the notion of memory. So, what this means now is that the network output predictions and the computations are not only the function of the input at a time step but also the memory of the cell state denoted by h_t. Thus, the output depends both on the current input as well as the past computations and the past learning that occurs. One can define this relationship by means of functions that map the input to the outputs as below;</p><div class="slate-resizable-image-embed slate-image-embed__resize-middle"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQHTqSjl_84Y6w" src="https://media.licdn.com/dms/image/C4D12AQHTqSjl_84Y6w/article-inline_image-shrink_400_744/0/1629645556407?e=1691625600&amp;v=beta&amp;t=FE-lXQ5O6uo918_0--euKMUmHl0WWWQ9-F1XU9L5GSs"></div><p><br></p><p>As we can see that we can describe the neurons via a recurrence relation which means the cell state depends upon the current input and again on prior cell states. It is exactly this idea (see figure&nbsp;below) of recurrence relation that provides the intuition of the key operation behind the recurrent neural networks or RNNs and through the sections 3 and 4 below we build upon the understanding of the mathematics of the recurrence relation and the operations that define the RNN behaviour.</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQFy100uKo1Qsw" src="https://media.licdn.com/dms/image/C4D12AQFy100uKo1Qsw/article-inline_image-shrink_1000_1488/0/1629645681532?e=1691625600&amp;v=beta&amp;t=kLCGjGENk5Qnnqavf5E4zGdgrLRMrYIfldjzpL4mGGc"></div><p><strong>                                 Figure: The Recurrence Relation in RNNs</strong></p><p><strong>&nbsp;</strong></p><h2>3.&nbsp;&nbsp;&nbsp;Recurrent Neural Networks:</h2><p><strong>&nbsp;</strong></p><p>Now, let us formalize the discussion a bit. The key idea is that the RNNs as described above maintain an internal state defined by h_t which is updated at each time step as the sequence is processed and this is done by the recurrence relation which specifically defines how the state is updated at the time step. Specifically, we define the internal cell state h_t;</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQGfJZSgqpj7JA" src="https://media.licdn.com/dms/image/C4D12AQGfJZSgqpj7JA/article-inline_image-shrink_400_744/0/1629645873746?e=1691625600&amp;v=beta&amp;t=NW4RU_No_zetghozr-f1cwl4DJTnQdyaop5sJykD048"></div><p>This internal cell state is defined by a function that can be parametrized by a set of weights “w”. These weights w will be learnt during training such a network. The function f_w is going to take as input both: the input at the current time step x_t as well as the prior cell state h_t-1;</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQFpXRoyEGAsqw" src="https://media.licdn.com/dms/image/C4D12AQFpXRoyEGAsqw/article-inline_image-shrink_400_744/0/1629645893065?e=1691625600&amp;v=beta&amp;t=UdnhtilnmeKsxc-drLgD_gU3TQgJvyv6FTnhWLe-df8"></div><p>The key feature of the RNNs is that they use the same function and the same parameters whilst processing the sequence and of course the weights change during the course of the training but during each iteration of the training the same set of weights are going to be applied across each of individual time steps.</p><h2>3.1&nbsp;RNN Computation: State Update and Output</h2><p><br></p><p>RNN computations include both the internal cell state update h_t as well as the output prediction itself. Let us now walk-through how these RNN computations are defined; </p><p>Firstly, we are going to consider the input vector x_t and then we are going to update the hidden state, the updated hidden state is;</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQGcCOrPQlAGnA" src="https://media.licdn.com/dms/image/C4D12AQGcCOrPQlAGnA/article-inline_image-shrink_1000_1488/0/1629646203652?e=1691625600&amp;v=beta&amp;t=Isx-ofS7XbyQrQq_aTmLZ1oo6hx4rnuBic2-vzwqEQw"></div><p>As seen in the above equation; the function used in the calculation of the hidden state is a standard neural network function as seen in the beginning of the section 2. Again, as evident from the above equation, this internal cell state h_t is going to depend upon both: the input x_t and previous cell state h_t-1 and we are going to multiply each of the terms by their respective weight matrices and we are going to add the result and apply a non-linear activation function to the sum of the 2 terms. The non-linear activation function here is going to be the hyperbolic tangent.</p><p><br></p><p>Then to generate the output at a given time step we take the internal hidden state at a given time step ‘’t’ and multiply by a separate weight matrix which inherently produces a modified version of the hidden state which actually forms the output prediction;</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQFl0AQFBU7RXQ" src="https://media.licdn.com/dms/image/C4D12AQFl0AQFBU7RXQ/article-inline_image-shrink_400_744/0/1629646259682?e=1691625600&amp;v=beta&amp;t=cbBmFfZbUJ_-j9CtSd10sxPpwrbtbdfuB0UaYZIGtN4"></div><p>This gives the mathematics of how an RNN updates its hidden state and produces the predicted output. The RNNs described above can be represented as below;</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQFaxVmHq8hIVQ" src="https://media.licdn.com/dms/image/C4D12AQFaxVmHq8hIVQ/article-inline_image-shrink_1000_1488/0/1629646310779?e=1691625600&amp;v=beta&amp;t=pK1f-_BIsSRN44MkS6fYEcs081SyioPYm8DngPt-QAA"></div><p>                                    <strong>Figure: Representation of RNN</strong></p><p>It should be emphasized that in a RNN we will be re-using the same weight matrices at every time step.</p><p><br></p><h2>3.2&nbsp;RNN Computation: Computing loss in RNNs</h2><p><br></p><p>Now, when we make a forward pass through a network we’re going to generate outputs at each of the time steps and from individual time steps we can derive a value for the loss and then the sum the losses over individual time steps together to determine the total loss which ultimately will be used to train the RNN. </p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQEM7VPwNihD5A" src="https://media.licdn.com/dms/image/C4D12AQEM7VPwNihD5A/article-inline_image-shrink_1000_1488/0/1629646393776?e=1691625600&amp;v=beta&amp;t=0ziIlg8i1mjEKxnnqr4QPrb95cL3tlkew2wiSfKLOGk"></div><p><strong>                                      Figure: Computing losses in an RNN</strong></p><p><br></p><h2>3.2&nbsp;RNN Computation: Computing loss in RNNs</h2><p>The screenshot below shows how RNNS are implemented in the Tensor Flow framework;</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQFDTRFeyMSF8w" src="https://media.licdn.com/dms/image/C4D12AQFDTRFeyMSF8w/article-inline_image-shrink_1000_1488/0/1629646474344?e=1691625600&amp;v=beta&amp;t=Ybz2LYPGt7lJ3YuU1D4aJgLv_sXT3intCKFFp6jnMj8"></div><p><strong>                           Figure: RNNs through a TensorFlow Code</strong></p><h2>4.&nbsp;&nbsp;&nbsp;RNNs for Sequence Modelling: The RNN Architecture</h2><p><br></p><p>Up-to this point, through sections 1,2 and 3, we have built our understanding and the mathematical basis of RNNs. It’s now time to turn back to the application of sequence modelling – because now one might have a sense of why RNNs are best suited to sequence modelling. </p><p><br></p><p><strong>RNN Architectures</strong></p><p>There are different types of RNN architectures, and these may be classified as;</p><p>o&nbsp;&nbsp;One-to-Many Architecture (or, Vector -to-Sequence Architecture)</p><p>o&nbsp;&nbsp;Many-to-One Architecture (or, Sequence -to-Vector Architecture)</p><p>o&nbsp;&nbsp;Many-to-Many Architecture (or, Sequence -to-Sequence Architecture)</p><p><strong>Example of One-to-Many Architecture:</strong></p><p>If we want a neural network to predict caption of an image describing its contents, then, this type of architecture is known as One-to-Many type of architecture because the RNN takes a single image as input and generates multiple words to describe it.&nbsp;</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQFT769rdl6KgQ" src="https://media.licdn.com/dms/image/C4D12AQFT769rdl6KgQ/article-inline_image-shrink_1000_1488/0/1629646738340?e=1691625600&amp;v=beta&amp;t=03iSv-q3uQwKFzpqHAkYSW8FuIj5RcyEWMsh32O6oPk"></div><p><strong>                       Figure: Example of One-To-Many Architecture – Caption generation</strong></p><p><strong>Example of Many-to-One Architecture </strong></p><p>Another type of RNN Architecture is the Many-to-One. One practical example of this type is sentiment analysis. In that instance, if you have a sequence of words, like this a sentence in the report states: <em>I'm very happy</em> as input, and your model should output whether the tweet has a negative or positive sentiment. The RNN would take every word from the sequence as inputs in different steps, propagate information from the beginning to the end, and output the sentiment. This is illustrated in the figure below;</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQHJTFArIwwQCg" src="https://media.licdn.com/dms/image/C4D12AQHJTFArIwwQCg/article-inline_image-shrink_1000_1488/0/1629646798468?e=1691625600&amp;v=beta&amp;t=jN9wr1to7gLIIDluquGO1pps2EemWWGL5Ql8TQ1TtJg"></div><p><strong>                          Figure: Many-to-One RNN Architecture</strong></p><p><strong>Example of Many-to-Many Architecture</strong></p><p>Finally, many-to-many tasks involve multiple inputs and multiple outputs. For example, in machine translation, you have a sequence of words in French. You want to get its equivalent in another language, such as English. An RNN would do well for this task, because they propagate information from the beginning to end. And this is what makes them able to capture the general meaning of word sequences. This architecture, encoder-decoder, is very popular for machine translation. The first part of this neural network doesn't return any output, y_hat, and is called an encoder. Because it encodes sequences of words in a single representation that captures the overall meaning of the sentence, that is decoded later to a sequence of words in the other language.</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQG94mUPunwgyg" src="https://media.licdn.com/dms/image/C4D12AQG94mUPunwgyg/article-inline_image-shrink_1000_1488/0/1629646928267?e=1691625600&amp;v=beta&amp;t=PDa-wmbQMnQMn4kHGwfiJHJf85b_C0TxlbtgwlH-uyM"></div><p><strong>                 Figure: Many-to-Many RNN Architecture</strong></p><p><br></p><p>Beyond the above, one can extend recurrent neural networks to many other applications wherein sequence processing and sequence modelling can be useful.</p><p><br></p><h2>5.&nbsp;&nbsp;&nbsp;Sequence Modelling: Design Criteria</h2><p><br></p><p>In order to appreciate why sequence modelling may be useful it is important highlighting – what may be termed as design criteria for sequence modelling as described below;</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;i.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Specifically, it is important we keep in mind that any recurrent neural network or machine learning g model we’re interested in should be equipped to handle variable length sequences because not all sequences are going to have the same length so we need to have the ability to handle variable length sequences</p><p><br></p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ii.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We also need to have the critical property to handle long term dependencies of data</p><p><br></p><p>&nbsp;&nbsp;&nbsp;&nbsp;iii.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;And then, to have the notion of memory and associated with that have a sense of order and a sense of how that occurred previously or earlier in the sequence affect on what’s going to happened later</p><p><br></p><p>&nbsp;&nbsp;&nbsp;&nbsp;iv.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The points ii and iii above are mathematically achieved by sharing the weights through every time step.</p><p><br></p><p>Recurrent neural networks meet the above design criteria to a large extend and are well suited to sequence modelling – subject to limitations that are discussed in section 8. Limitations of recurrent neural networks are overcome by Long Short-Term Memory (LSTM) units and Attention discussed in section 11 onward</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQEWE1JCvlEqVg" src="https://media.licdn.com/dms/image/C4D12AQEWE1JCvlEqVg/article-inline_image-shrink_1000_1488/0/1629647028757?e=1691625600&amp;v=beta&amp;t=UA6ihrN48lMEXyaDeAZ03lcExkfoR8lSPHMGeo9O0r4"></div><p><strong>                         Figure: Sequence Modelling Design Criteria</strong></p><p><br></p><h2>6.&nbsp;&nbsp;&nbsp;A Sequence Modelling Problem: Predicting the Next Word</h2><p><br></p><p>Let us consider a very concrete sequence modelling problem which is going to be: given a series of words in a sentence we will have to predict the next word to occur in the sentence</p><p>E.g. Let us say we have this sentence as an example;</p><p>“The bolt hole stress was less than the material yield-stress”</p><p><br></p><p>And the task is given the words:</p><p>“The Bolt hole stress is less than the material -----------------------”</p><p>We must predict the word “yield-stress”</p><p><br></p><p>Now, the first consideration before we even start training the model is to represent language to a neural network and this is discussed below.</p><p><strong>&nbsp;</strong></p><h2>6.1&nbsp;Representing Language to a Neural Network</h2><p><br></p><p>The first step before even getting started is how to represent language to a neural network. It should be underscored that neural networks are functional operators – they execute functional mathematical operations on the inputs and generate numerical outputs.</p><p>As neural network or computers cannot understand words and can only interpret numbers, thus, neural networks require numerical inputs that can be a vector or array of numbers so that the model can operate on them and generate a vector or array of numbers as output.</p><p>Let us say we have a 2-word sentence: “Deep Learning” – given he word “Deep” we must predict the word “Learning”. As described above, we cannot pass words through the networks but we need a vector or array of numbers corresponding to the word as illustrated below;</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQHPLSsH_oY-fQ" src="https://media.licdn.com/dms/image/C4D12AQHPLSsH_oY-fQ/article-inline_image-shrink_1000_1488/0/1629647257654?e=1691625600&amp;v=beta&amp;t=JadyqKs-oq9suC3N1r9zYAU52pyL1WcxEe4TfMKMbiw"></div><p><strong>                         Figure: Representing language to a neural network</strong></p><p><strong>&nbsp;</strong></p><p>Thus, as illustrated above we must transform language into an array-based representation. The solution we are going to consider is to represent words as “numbers” is the concept of “embeddings” which is the idea of transforming a set of indices into a vector of fixed size that captures the content of the input. In order to understand how this can be done for the language data, let us consider the same example;</p><p><br></p><p>                           “The bolt hole stress was less than the material yield-stress”</p><p>Here;</p><p>We want to be able to map every word that appears or could have appeared in the body of the language to a fixed size vector. So, the first step is to generate a vocabulary (see figure below) which is going to consist of all unique words in the set of language. We can then index all the unique (again in the figure below) words by mapping each word to unique indices and these indices can then be mapped to vector embeddings. One way we could do this by is by generating sparse and binary vectors that is going to have a length equal to the length of the vocabulary such that we can indicate a particular word by encoding it in a corresponding index as shown below. This is called “one hot encoding”&nbsp;</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQGFYz8rQdpTXg" src="https://media.licdn.com/dms/image/C4D12AQGFYz8rQdpTXg/article-inline_image-shrink_1000_1488/0/1629648298982?e=1691625600&amp;v=beta&amp;t=CASKc1OvMRzumVu--ZVdSwgvC4cGOo3eks7gl3P1hek"></div><p><strong>                                           Figure: Vocabulary – corpus of words</strong></p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQECoSjGedWKEw" src="https://media.licdn.com/dms/image/C4D12AQECoSjGedWKEw/article-inline_image-shrink_400_744/0/1629648327795?e=1691625600&amp;v=beta&amp;t=8sKUlxxU58YgKdT2izb7nm2Q5w0r5gyh0guArIIwzKM"></div><p><strong>                                     Figure: Indexing – word to index</strong></p><p><br></p><p>Another way to build up the embedding is by learning them. The idea here is to take the index mapping and to feed the index mapping into a model like neural network such that we can transform the index mapping across all the words of the vocabulary to a vector of lower dimensional space where the values of the vectors are learnt so that the words similar to each other have similar embeddings. An example demonstrating this concept is shown below. Thus, there are 2 distinct ways we can encode language data and transform language data into a vector representation that can best suited for input to a neural network.</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQHvCNL3kckY2g" src="https://media.licdn.com/dms/image/C4D12AQHvCNL3kckY2g/article-inline_image-shrink_1000_1488/0/1629648471892?e=1691625600&amp;v=beta&amp;t=k0JaYZsEvC8_03oRwjnXOS0HidCStXlsZIjHXbCgBTM"></div><p><strong>                                      Figure: Embedding: Index to fixed size vector</strong></p><p><br></p><h2>6.2&nbsp;Why RNNs meet the design criteria?</h2><p><br></p><p>Now that we’ve built up the way to encode language data and started getting the feeling of the recurrent neural network model, let is go back to the set of design criteria and understand if the RNNs meet the design criteria;</p><p><br></p><p>1.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The first design criteria with regards to the recurrent neural networks was that the RNNs should have the ability to handle variable sequence length. Let us consider the example of predicting the next word and consider the variable sequence of the length of the sentences as shown below;</p><p>                                        The bolt hole <em><u>stress</u></em></p><p>                                                    vs.</p><p><em>The bolt-hole stress was less than the material <u>yield-stress</u></em></p><p>                                                        Vs</p><p><em>Since the bolt-hole stresses were considerable low, the low cycle fatigue life of           the bolt hole was significantly   <u>high</u></em></p><p><br></p><p>Recurrent neural networks can handle variable sequence lengths. Feed forward networks are not able to do this because they have inputs of fixed dimensionality and then those inputs are passed onto the next layer. In contrast to this RNNs can handle variable sequence lengths and that’s because the differences in sequence lengths are just the differences in number of time steps that are going to be input and processed by the RNN. So, RNNs meet the first design criterion of handling variable sequence lengths.</p><p><br></p><p>2. The second design criteria being the ability to handle long term dependencies which is emphasized by this example below wherein we need information from the <strong>distant past</strong> to accurately predict the correct word. RNNs can achieve this because they can update their&nbsp;internal cell state via the recurrent relation which fundamentally incorporates the information from the past state into the cell state update.</p><p><br></p><p><strong><em>France</em></strong><em> is where I grew up, but I now live in Boston. I speak fluent …….</em></p><p><em>&nbsp;</em></p><p>3.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;RNNs also capture the differences in sequence order which could result in difference in the overall meaning of the sentence. For example, consider the following example;</p><p><br></p><p><em>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The bolt hole stress was insignificant and hence this feature was able meet the LCF life target</em></p><p><em>&nbsp;                                                                  Vs.</em></p><p><em>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The bolt hole stress was significant and hence this feature was unable meet the LCF life target</em></p><p><em>&nbsp;</em></p><p><br></p><p>Here the 2 sentences have opposite semantic meaning but have the same words with the same counts just in different order and again the cell state maintained by an RNN depends on the past history which helps in capturing the difference and also because we’re maintaining the information of the past history and also maintaining the same weight matrices across each of the independent time steps in the sequence.</p><p><br></p><p>Thus, it is evident from the discussion above how sequences can be represented and encoded for inputs to RNNs and how RNNs can achieve the sequence modelling design criteria. </p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQHoa_raJtF4Rg" src="https://media.licdn.com/dms/image/C4D12AQHoa_raJtF4Rg/article-inline_image-shrink_1000_1488/0/1629648770776?e=1691625600&amp;v=beta&amp;t=rdmHc3XItlF5IS7LW_gmCWyAyla1j0Ja8eveMCXYZ8Q"></div><p><strong>Figure: Recurrent neural networks meet the sequence modelling design criteria</strong></p><p><br></p><h2>7.&nbsp;&nbsp;&nbsp;Training the RNNs</h2><p>Now that we’ve built up our understanding on how recurrent neural networks work, how they operate and what it means to model sequences. Let us now discuss the algorithm on how we can train recurrent neural networks. This is twist to the backpropagation algorithm used for conventional neural networks and is called as “Back-propagation through time”</p><p><br></p><h2>Recall: Back propagation in feed forward models</h2><p>Let us take a step back and see how we train feed forward models using the back-propagation algorithm. We first take a set of inputs and make a forward pass through the network going from the input to the output. And then to train the model we back propagate the gradients back through the network and take the derivative of the loss with respect to each weight parameter in our network and then adjust the parameters – the weights in the model in order to minimize the loss.</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQHeoob9jl9v8w" src="https://media.licdn.com/dms/image/C4D12AQHeoob9jl9v8w/article-inline_image-shrink_1000_1488/0/1629648889587?e=1691625600&amp;v=beta&amp;t=f_hYsq9HYB1rn-OVnbmhiYPdVDzXcfz42hnIZvwruac"></div><p><strong>                                  Figure: Back-propagation in feed forward models</strong></p><p><br></p><p>For RNNs, as it was pointed out earlier, the forward pass through the network consists of going forward across time and updating the cell state based on the input as well as the previous state and then generating the output. The loss is computed at each time step and then finally the individual losses are summed to get the total loss.</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQGPdpOfPX0fDw" src="https://media.licdn.com/dms/image/C4D12AQGPdpOfPX0fDw/article-inline_image-shrink_1000_1488/0/1629648985459?e=1691625600&amp;v=beta&amp;t=D-JTJz8SEq0HuQ2eGRAs8o97zBGwAxObanUp-1m2hUw"></div><p><strong>                                    Figure: RNNs – back-propagation through time</strong></p><p><br></p><p>Thus, instead of back-propagating the error through a single feed forward network and a single time step; in RNNs those errors are back-propagated from the overall loss through each individual time step and then across all the time steps all the way from where we’re currently in the sequence back to the beginning . Thus, it is called back-propagation through time because all of the errors are going to flow back in time from the most recent time step to the very beginning of the sequences.</p><p><br></p><h2>8.&nbsp;&nbsp;&nbsp;Problems in Recurrent Neural Networks</h2><p><br></p><p><strong>The Pros and Cons of RNNs</strong></p><p>As stated above, RNNs capture dependencies within a short range and take up much less RAM than N-gram models. Regarding, the problems with RNN, the first fundamental issue to be highlighted is the how the vanishing or exploding gradients are created which can cause the model training to fail. </p><p><br></p><p><strong>The Vanishing and the Exploding Gradients: </strong></p><p><br></p><p>Vanishing / exploding gradients are a problem which can arise since RNNs propagates information from the beginning of the sequence through to the end. </p><p>Starting with the first word of the sequence, the hidden value at the far left, the first values are computed here. Then it propagates some of the computed information, takes the second word in the sequence, and gets new values. One can see that process illustrated here, the orange area denotes the first computed values, and the green denotes the second word. </p><p>The second (hidden) values are computed using the older values in orange, and the new word in green. After that, it takes the third word and propagates the values from the first and second words and computes another set of values from both of those, and it continues in a similar way from there.&nbsp;</p><p>At the final step, the computations contain information from all the words in the sequence, and the RNN can predict the next word, which in this example is <em>goal</em>. Know that in an RNN, the information from the first step doesn't have much influence on the outputs. Therefore, you can see the orange portion from the first step decreasing with each new step. Correspondingly, computations made at the first step don't have much influence on the cost function either. </p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQFLyjVgmtc3zg" src="https://media.licdn.com/dms/image/C4D12AQFLyjVgmtc3zg/article-inline_image-shrink_1000_1488/0/1629649795738?e=1691625600&amp;v=beta&amp;t=93hLOg9og9QM2mMjBvGoPiqZQ8n-BwO0J-tin6lT6vw"></div><p><br></p><p><strong>        Figure: The propagation of information through an RNN</strong></p><p><br></p><p><strong>&nbsp;</strong></p><p><strong>Understanding the Vanishing and Exploding Gradient problem in an RNN</strong></p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQHt3QeWasbl0g" src="https://media.licdn.com/dms/image/C4D12AQHt3QeWasbl0g/article-inline_image-shrink_1500_2232/0/1629649816960?e=1691625600&amp;v=beta&amp;t=GsgsMoqPmhpsTa72jNTP1lZghFpK8Bts39KYgmX9_Mg"></div><p><br></p><p><strong>                                   Figure: The Vanishing Gradient problem in RNNs</strong></p><p><br></p><p>We know that gradients are calculated during back propagation i.e. during the process of moving backwards towards the initial layer from the final layer that was reached during the forward pass. The derivatives from each layer are then multiplied from back to front in order to compute the derivative of the initial layer. The gradients may be thought of as a measure of how a model can improve over a series of time steps. When the network performs back propagation, the gradient of the loss with respect to the weight of that time step is calculated. But in a network with many time steps having a gradient arrive back at the early layers as the product of all the terms from the latter layers makes an inherently unstable solution;</p><p>That is, consider the following gradient computed for an initial layer as a product of the terms in the latter layers;</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQEZx1G1XQNg_A" src="https://media.licdn.com/dms/image/C4D12AQEZx1G1XQNg_A/article-inline_image-shrink_400_744/0/1629649909648?e=1691625600&amp;v=beta&amp;t=IFscHxeU8SPbXmo1XFSwfP7Bax-RvN_0acezomF0FAQ"></div><p>Thus, more the layers, more will be terms. Suppose if the gradient is &lt; 1 then further multiplication with more terms makes the gradient still smaller and then multiplying with the learning rate which has values between 0.0001 to 0.01 will further reduce the value! This will result in very minute gradient values which have negligible effect on the weight update – that is at this point wherein the gradients are too small to have any effect on the update of the weight the network stops learning and this is the problem of the <strong><em>“vanishing gradient” </em></strong>which is a signature of larger networks<strong><em> </em></strong></p><p><br></p><p><strong>Other problems in RNNs – computational inefficiency</strong></p><p><strong>&nbsp;</strong></p><p>Further, RNNs are slow to train, the inputs data needs to be passed sequentially or serially one after the other. We need input data from the previous state to process any output from the current state. Further, RNNs cannot deal with longer sequences which leads to the problem of vanishing and/or exploding gradients as elaborated above. The problem is solved – to an extent – through the Long-Term Short-Term Memory units – termed as LSTMs which are discussed next in this article/primer.</p><p><strong>&nbsp;</strong></p><h2>9.&nbsp;&nbsp;&nbsp;Solving the Vanishing Gradient Problem – Long Short-Term Memory Units (LSTMs)</h2><p><br></p><p>One way to tackle the long term dependencies and track information across multiple time steps and try and overcome the vanishing gradient problem is to use a ‘more complex’ recurrent unit with gates which can be thought as (intuitively) as one that controls the information that is passed through. Specifically, we’re going to use what is called as “gated” cell and particular type of gated cell that is used in recurrent neural network is the “long short-term memory (lstm)” gated cell.</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQEuV2EpIvG3iQ" src="https://media.licdn.com/dms/image/C4D12AQEuV2EpIvG3iQ/article-inline_image-shrink_1000_1488/0/1629650006830?e=1691625600&amp;v=beta&amp;t=Owd9J4UGdiAvtRda5IwiDaKtDLPXH_V4oggDVWKgB34"></div><p><strong>                  Figure: Gated cell in Long Short-Term Memory Unit (LSTM)</strong></p><p>Let us get an intuition of the fundamental operations of LSTM whilst moving away from the math:</p><p><br></p><p>To understand the key operations that make the LSTMs special, let us go back to the structure of a RNN – we have as shown in the figure below a representation of the RNN that shows the illustration of the operations that define the state and the update output functions. Here, in the figure below we have the black lines that effectively capture the weight matrix multiplications and the yellow rectangles such as tan h depicts the non-linear activation functions.</p><p><br></p><p>Thus, here in the diagram each repeated module of RNN contains a single compute neural network node which comprises of a tan h activation layer. </p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQGFw69nNBz8dQ" src="https://media.licdn.com/dms/image/C4D12AQGFw69nNBz8dQ/article-inline_image-shrink_1000_1488/0/1629705133663?e=1691625600&amp;v=beta&amp;t=S-h0xpvjvDgElm4PotVBWY-_FQ1ZYYs-saBO8DB169Y"></div><p><strong>Figure - RNNs: Repeating modules contain a single computation node</strong></p><p><br></p><p>We perform an update to the internal cell state ht which is going to depend upon the previous cell state ht-1 as well as the current input xt and at each time step, we’re going to generate an output prediction yt – a transformation of the state.LSTMs have a chain like structure, the recurrent repeating module – the recurrent unit is slightly more complex. </p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQHmUvoSQAS5MA" src="https://media.licdn.com/dms/image/C4D12AQHmUvoSQAS5MA/article-inline_image-shrink_1000_1488/0/1629705154203?e=1691625600&amp;v=beta&amp;t=6wO6yPL9sRsit3hGbf8veXjXVA0LRKLkodteVorEqjs"></div><p><strong>Figure: LSTM modules – computational blocks that control information flow</strong></p><p><br></p><p>In the LSTMs, the repeating recurrent unit contains the different interacting layers which are defined by the standard neural network operations with sigmoid or tan h non-linear activation functions with weight matrix multiplication. The importance of these different interacting layers is that they can effectively control the flow of information through the LSTM cell and the paragraphs will highlight how the updates enable these LSTMs track and store information through many time steps.</p><h2>9.1&nbsp;Key Ideas in Long-Short-Term-Memory-Units (LSTMs)</h2><p><br></p><p>The key idea behind LSTMs is that they can selectively add or remove information to and from the internal cell states using structures called “Gates”. The Gates contain standard neural network layer like the “sigmoid” and pointwise multiplication</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQGCQBfQUVsi1g" src="https://media.licdn.com/dms/image/C4D12AQGCQBfQUVsi1g/article-inline_image-shrink_400_744/0/1629705220659?e=1691625600&amp;v=beta&amp;t=W76yEL-6CskbhExf07EWGZcaakjkcvD_z3hBnceIWEg"></div><p><br></p><p><br></p><p><strong>Figure: LSTMs – information is added or removed through structures called “gates”</strong></p><p><br></p><p>Now, let us see what these gates are doing. For example, we have a sigmoidal activation function&nbsp;- this is going to force anything that passes through the gate to be between 0 and 1. One can think of think of this as modulating and capturing how much of the input should be passed through between nothing [0] and everything [1] which effectively “gates” the flow of information.&nbsp;LSTMs use this type of operation by first forgetting the irrelevant history secondly by storing the relevant information and thirdly by updating the internal cell state and finally (fourth) updating the output.&nbsp;</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQEyhghGaW2xoQ" src="https://media.licdn.com/dms/image/C4D12AQEyhghGaW2xoQ/article-inline_image-shrink_1000_1488/0/1629705269492?e=1691625600&amp;v=beta&amp;t=tBJid0NA3FtqetKllPoN8Vf17RjYRnZtF3Z4203Hzpo"></div><p><strong>                                      Figure: How LSTMs work</strong></p><p><br></p><p>The steps describing how LSTMs process information are described below;</p><p>1.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The first step is to forget the irrelevant parts of the previous state and this is achieved by taking the previous state and passing it through one of the sigmoid gates which can be thought of as a step modulating the flow by how much information should be passed or kept out.</p><p><br></p><p>2.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The next step is to find out what part of the new information and what part of the old information is relevant, and this stored into the cell state</p><p><br></p><p>3.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The highlight of the LSTMs is that they maintain a separate value of the cell state ct in addition to what was introduced previously ht. The ct is going to be selectively updated by the gated operations</p><p><br></p><p>4.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Finally we can return an output from the LSTM – this is an interacting layer – the output gate that can control what information that’s encoded in the cell state is ultimately outputted and sent to the network as input in the following time step. This operation controls the value of the output yt as well as the cell state that’s passed from time step to time step in the form of ht.</p><p><br></p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQEkQs_IrLMRWw" src="https://media.licdn.com/dms/image/C4D12AQEkQs_IrLMRWw/article-inline_image-shrink_1000_1488/0/1629744193119?e=1691625600&amp;v=beta&amp;t=yFrYbH5le-e7ZVcBtreZupr8DQK4sBc9BVSs3OSINlA"></div><p><strong> Figure: LSTM Process – Step 1 – ‘Forget’: LSTMs forget irrelevant parts                    of previous state</strong></p><p><br></p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQFs89O3jgToGw" src="https://media.licdn.com/dms/image/C4D12AQFs89O3jgToGw/article-inline_image-shrink_1000_1488/0/1629744246136?e=1691625600&amp;v=beta&amp;t=OjD-Tc8yRNUPXpSNoDgFfjPhJnNCGyK6gTJ_48p2WvM"></div><p><strong>&nbsp;Figure: LSTM Process – Step 2 – ‘Store’: LSTMS store relevant new information into the cell state</strong></p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQF6yEfeXQts-w" src="https://media.licdn.com/dms/image/C4D12AQF6yEfeXQts-w/article-inline_image-shrink_1000_1488/0/1629744415867?e=1691625600&amp;v=beta&amp;t=H847AIa3XsL56rCmAuEdpRRH6OAz0r7RsRkqAh4eOcs"></div><p><strong>Figure: LSTM Process – Step 3 – ‘Update’: &nbsp;LSTMs selectively update cell state values</strong></p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQGkpJZMYbx6zg" src="https://media.licdn.com/dms/image/C4D12AQGkpJZMYbx6zg/article-inline_image-shrink_1000_1488/0/1629744456759?e=1691625600&amp;v=beta&amp;t=DbhAX31F6S0VrgOBvPZGbeMjpsRLrraUQz52-OC-sRE"></div><p><strong>Figure: LSTM Process – Step 4 – ‘Output’:&nbsp;Output gate controls information passed to the next time step</strong></p><p>Key take away points from LSTMs:</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The key-take away from LSTMs is that they can regulate information flow in storage thus they can effectively better capture long term dependencies and overcome the vanishing gradient problem. LSTMs maintain a separate cell state from what is outputted that is ct</p><p><br></p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The use of the gates to control the flow of information by: forgetting the irrelevant information from the past history, storing the relevant information from the current input, updating the cell state and outputting the information at each time step.</p><p><br></p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;It is the maintenance of the separate cell state which allows the back propagation through time with uninterrupted gradient flow and more effective training. Thus, LSTMs are commonly used in deep learning practise – until recently – where they have been largely replaced by Attention models which are discussed next!</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQGG0XH7BC4zIw" src="https://media.licdn.com/dms/image/C4E12AQGG0XH7BC4zIw/article-inline_image-shrink_1000_1488/0/1629917889052?e=1691625600&amp;v=beta&amp;t=wn6QxpsXVQ-ueKqVV40CmwS84slmvA4a2scB-tMNoWM"></div><p><strong>Figure: LSTM uninterrupted gradient flow</strong></p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQGx2ae1O3V0VQ" src="https://media.licdn.com/dms/image/C4E12AQGx2ae1O3V0VQ/article-inline_image-shrink_1000_1488/0/1629917921687?e=1691625600&amp;v=beta&amp;t=9tPKeG-lw0wOQPfzC2VzJxn_Syk2PO7o9PC7cteQMyI"></div><p><strong>Figure: LSTM uninterrupted gradient flow</strong></p><p><br></p><h2>10.&nbsp;&nbsp;&nbsp;Applications of RNNs</h2><p><br></p><p>Having covered the fundamental working of RNNs and introduce the back propagation with time algorithm and introduced the architecture of LSTMs, it might be useful to highlight an exciting example wherein one can build up a recurrent neural network&nbsp;that can take sequences of musical notes and from that sequence predict the most likely next note to occur and not only predict the most likely next note but also take the trained model and use it to generate brand new musical sequences that have never been heard before. This can be done by seeding a trained RNN model with a first note and iteratively build up a sequence over time to generate a new song and this indeed forms a powerful application of RNNs.</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQFe3nIcMMbhPQ" src="https://media.licdn.com/dms/image/C4E12AQFe3nIcMMbhPQ/article-inline_image-shrink_1000_1488/0/1629918348235?e=1691625600&amp;v=beta&amp;t=Fo4FhZesX5FK2OCfvdUlv0JFJB0Y0IwQp5aknrsaHBU"></div><p><strong>Figure: RNNs for music generation</strong></p><p><br></p><p>Here is an interesting example of the famous classical composers Franz Schubert: <a href="https://www.classicfm.com/composers/schubert/unfinished-symphony-completed-by-ai/" target="_blank">https://www.classicfm.com/composers/schubert/unfinished-symphony-completed-by-ai/</a></p><p><br></p><p>Another application of RNNs beyond music generation is the one in language processing where one can go from an input sequence like a sentence to a single output where we can train the RNN to produce a prediction or sentiment associated with a particular sentence either positive or negative which is effectively a classification task.</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQGK-PwoYHsXEw" src="https://media.licdn.com/dms/image/C4E12AQGK-PwoYHsXEw/article-inline_image-shrink_1000_1488/0/1629918383341?e=1691625600&amp;v=beta&amp;t=t67bMf9IbBNQeBBusRY-lu7ifjhRaVJoSxk-fW8eHSc"></div><p><strong>Figure: RNNs for sentiment classification</strong></p><p>The next example is another powerful application of RNNs and is the backbone of google translate&nbsp;and that’s the idea of the machine translation wherein the goal is to input a sentence in one language and train the RNN to output the sentence in another language and this can be done by having an encoder component which effectively encodes the original sentence into some state vector and a decoder component which decodes the state vector into the target language.</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQG3e8-rf43GQQ" src="https://media.licdn.com/dms/image/C4E12AQG3e8-rf43GQQ/article-inline_image-shrink_1000_1488/0/1629918429427?e=1691625600&amp;v=beta&amp;t=Q-eu173jcsswczCmrRO3J4k_zxumDZFAQA3YZAZDyPE"></div><p><strong>Figure: RNNs for machine translation</strong></p><p><strong>&nbsp;</strong></p><h2>Potential issues with RNNs</h2><p><strong>&nbsp;</strong></p><p>RNNs are slow to train - the input data needs to be passed sequentially or serially one after the other. We need inputs of the previous state to make any operations on the current state. Such sequential flow does not make use of the GPUs which are essentially designed for parallel computation. Further, RNNs cannot deal with long sequences very well which leads to the problem of vanishing and exploding gradients as explained above. LSTMs solve the problem of long sequences to an <em>extent,</em> but they are slower than RNNs!</p><p>This led to the discovery of ‘Transformers’ which are discussed in the next section.</p><p><br></p><p><br></p><p><br></p><p><br></p><h2>11.&nbsp;&nbsp;&nbsp;Transformer Neural Network Architecture</h2><p><br></p><p>In 2017, the Transformer Neural Network architecture was introduced. The network employs an encoder-decoder architecture much like the recurrent neural network. The following are some of the starting differences between the RNNs and the Transformer architecture;</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;i.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In an RNN encoder, we pass the input sentence one word after the other. The current word’s hidden state has dependencies on the previous word’s hidden state, the word embeddings are generated one word at a time.</p><p><br></p><p>Whereas, with a transformer encoder, there is no concept of “time step” – we pass in all the words of the sentence simultaneously and determine the “word embedding” simultaneously as shown in the figure below;</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQGApBF7DjxkxQ" src="https://media.licdn.com/dms/image/C4E12AQGApBF7DjxkxQ/article-inline_image-shrink_1000_1488/0/1629918519244?e=1691625600&amp;v=beta&amp;t=Nvs2ITtQt1sgbRjQ3gAbrAlDwgD2qRG6a6XevxMXecY"></div><p><br></p><p><strong>Figure: RNN vs Transformer – all words of the sentence passed simultaneously in Transformer model</strong></p><p><strong>How does the Transformer do this? The Transformer Architecture: </strong></p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQFnCAEgCaJjxQ" src="https://media.licdn.com/dms/image/C4E12AQFnCAEgCaJjxQ/article-inline_image-shrink_1000_1488/0/1629918819471?e=1691625600&amp;v=beta&amp;t=B4HpzkUE26MPjJEqneuN_z6zT5xODiyZdeKpB8tdHw4"></div><p><strong>Figure: The Transformer Architecture</strong></p><p><strong>&nbsp;</strong></p><p><strong>Input Embeddings: What are they?</strong></p><p><strong>&nbsp;</strong></p><p><em>[This has been discussed in section 6.1 and the purpose of this section is to highlight the above Glove]</em></p><p><br></p><p>Computers do not understand “words”, they only understand numbers, they get vectors and matrices. The idea is to map every word to a point in space so that similar words in meaning are physically closer to one another. The space in which they are present is called the “embedding space” and we would pre-train this embedding space to save time or even just use and already pre-trained embedding space as <strong>GloVe: Global Vectors for Word Representation</strong></p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQGnbDGSyBvScQ" src="https://media.licdn.com/dms/image/C4E12AQGnbDGSyBvScQ/article-inline_image-shrink_1000_1488/0/1629918886954?e=1691625600&amp;v=beta&amp;t=64YhpS0kU5ouz8ifmpQzNZiDvua7mEZo9LHqVuLEFgg"></div><p><br></p><p><strong>&nbsp;Figure: Understanding the Embedding Space</strong></p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQG_F1RsS-Xmcg" src="https://media.licdn.com/dms/image/C4E12AQG_F1RsS-Xmcg/article-inline_image-shrink_1000_1488/0/1629918909244?e=1691625600&amp;v=beta&amp;t=-cB3oWwkPoiJ7uDf0d7R3UmCSmBxXDMn0eDpK1kHtLc"></div><p><strong>&nbsp;Figure: Pre-trained Embedding Space - GloVe – Global Vectors for Word Representation</strong></p><p>This embedding maps a word to a vector but the same word in different sentences may have different meaning and this is where positional encoders come in the way which is a vector that has information on distances between the words. The original paper uses a sine and cosine function to generate this vector, but it could be any reasonable function. Thus, after passing English sentence and applying positional encoding we get word vector that has positional information.</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQG6y5VDChclqQ" src="https://media.licdn.com/dms/image/C4E12AQG6y5VDChclqQ/article-inline_image-shrink_1000_1488/0/1629919028036?e=1691625600&amp;v=beta&amp;t=xSQXQPg0s2y92b7P6RxiMCsi7AXrYN8qMS82eiGHoWo"></div><p><br></p><p><strong>Figure: Positional encoding </strong></p><p><strong>&nbsp;</strong></p><p><strong>&nbsp;</strong>After passing the English sentence through the input embedding and applying the positional encoding we get word vectors that have positional information: that is “context”</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQFdc_qrmXTUng" src="https://media.licdn.com/dms/image/C4E12AQFdc_qrmXTUng/article-inline_image-shrink_1000_1488/0/1629919089281?e=1691625600&amp;v=beta&amp;t=t29zoTSwgYcYnGFF8eBJEsZgHxRfTlFbe7lxTYACk-A"></div><p><br></p><p><strong>Figure: Embedding including position of the word in the sentence</strong></p><p><strong>&nbsp;</strong></p><p><strong>&nbsp;Encoder Block</strong></p><p><strong>&nbsp;</strong>Next, we pass this information through an Encoder block where it goes through a multi-headed <strong>Attention Layer </strong>and a <strong>Feed Forward Layer. </strong>Let us now understand the <strong>“Attention layer</strong>” and the <strong>“Feed Forward Layer” in </strong>some detail;</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQG9cFdXeoGe2g" src="https://media.licdn.com/dms/image/C4E12AQG9cFdXeoGe2g/article-inline_image-shrink_1000_1488/0/1629919379458?e=1691625600&amp;v=beta&amp;t=9hTfhFKMGJcc_FgUgQ_h_u9jYCAXzH8lNpq8yejnKz4"></div><p><br></p><p><strong>Figure: Encoder Block</strong></p><p><strong>&nbsp;</strong></p><p><strong>&nbsp;Attention Layer: </strong></p><p>It involves answering the question, what part of the input should we focus on. That is: if we’re translating from English to French and we’re doing self-attention that is attention with respect to oneself, then we want to answer the question, “how relevant is the ith word in the English sentence to other words in the same sentence”. This is computed in the attention block for every word in the sentence. Thus, for every word we can have an “attention vector” generated which captures contextual relationship between words in the sentence.</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQEsFj3O9ReDDw" src="https://media.licdn.com/dms/image/C4E12AQEsFj3O9ReDDw/article-inline_image-shrink_1000_1488/0/1629919413320?e=1691625600&amp;v=beta&amp;t=w5vIWZbrZLu_aC8E2_bbErWh_rY3OlL4nWu4EsTRLJU"></div><p><strong>Figure: Attention Block computing attention vectors for every word in the sentence</strong></p><p><strong>Feed forward unit:</strong></p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQHcTtyQCPNKPg" src="https://media.licdn.com/dms/image/C4E12AQHcTtyQCPNKPg/article-inline_image-shrink_1000_1488/0/1629919504985?e=1691625600&amp;v=beta&amp;t=1CtjZ1Ex1nfFV96df6Ig7Xp45WB8uKgJFNfsAPJjB9Y"></div><p><strong>Figure: Feed forward unit in the Encoder block</strong></p><p>The other important unit is the feed-forward unit – this is just a simple feed forward network which is applied to each of the attention vectors. These feed forward units are used I practise to transform the attention vectors into a form that is digestible by the next encoder block or the decoder block. This is a high-level overview of the encode block – next let us see the high-level overview of the decoder block before going into more details into the encoder block</p><p><strong>Decoder block:</strong></p><p>During the training phase of English to French translation, we feed the French sentence to the decoder. As mentioned before, <em>computers do not understand language</em>, they understand numbers, vectors and matrixes. So, we ‘process’ the language using the output embedding to get the vector form of the word and then add a positional vector to get the notion of context of the word in a&nbsp;sentence and we pass this vector finally into the decoder block</p><p><br></p><p>The decoder block has 3 main components;</p><p><br></p><p>Two of these components are like the encoder block – the self-attention and the feed forward unit. Like the encoder block the self-attention block generates the attention vectors for every word in the French sentence to represent how much every word is related to <em>each </em>word in the same sentence (just like the attention block in the encoder block). These attention vectors from the encoder block are passed onto another attention block, very often called as the “encoder-decoder” attention block because we have one vector from every word in the English sentence and another from the French sentence. This attention block determines <em>how the words in English and the French are related to each other – and this is where the English French mapping happens. </em>The output of this block is attention vectors for every word in the English and French sentence, each word representing the relationship with other words in both the languages.</p><p>Next, we pass the attention vector to the feed forward unit that makes the output vector digestible by the next decoder block precisely – the linear layer!</p><p><br></p><p>The Linear layer is used to expand the dimensions into the number of words int the French sentence. The softmax transforms it int a probability distribution which is now human interpretable, and the final word is the word corresponding to the highest probability. Overall, the decoder predicts the next word and we execute it over a multiple time steps until the final word is predicted.</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQHSob9C-Jnphw" src="https://media.licdn.com/dms/image/C4E12AQHSob9C-Jnphw/article-inline_image-shrink_1000_1488/0/1629919572939?e=1691625600&amp;v=beta&amp;t=w2oQ3oIFO0ZAr3a4roME8Y5WgW-E3gpK6MDnOxcn8Qc"></div><p><br></p><p><strong>Figure: The Decoder Block</strong></p><p><strong>&nbsp;</strong></p><h2>12.&nbsp;&nbsp;&nbsp;Details of the Encoder Block</h2><p><br></p><p>The encoding component is a stack of encoders- the original paper: <a href="https://arxiv.org/abs/1706.03762" target="_blank">https://arxiv.org/abs/1706.03762</a> stacks 6 of encoder blocks on top of each other – the decoding component is a stack of decoders of the same number.</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQEr43_UP3ItvA" src="https://media.licdn.com/dms/image/C4E12AQEr43_UP3ItvA/article-inline_image-shrink_1000_1488/0/1629919926323?e=1691625600&amp;v=beta&amp;t=IBpv1rEfrGSde0eeok7SphxYGKd0WtEA1wypdkuiNEg"></div><p><strong>Figure: Stack of encoder and decoder blocks</strong></p><p>The encoders are all identical in structure and yet they do not share same weights. <strong>Each encoder</strong> block is broken down into 2 sub-layers: self a-attention layer and the feed-forward layer as mentioned above. The decoder has both these layers but between them is the Encoder-Decoder attention layer which as described above. This is illustrated below for clarity;</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQFYGRHuUPVshg" src="https://media.licdn.com/dms/image/C4E12AQFYGRHuUPVshg/article-inline_image-shrink_1000_1488/0/1629919955721?e=1691625600&amp;v=beta&amp;t=IMNJGpzFcyeNc85uLH-nhk-xdU1xSVdiEXeAIA6kI5I"></div><p><strong>Figure: Contents of “each” encoder and “each” decoder block</strong></p><p><strong>&nbsp;</strong></p><p>The “embedding” only happens in the “bottommost” encoder which receive a list of vectors each of size 512 – which would be word embeddings as input. This happened in the bottommost encoder but in the other encoders – it would be the output of the encoders that is directly below. The size of the list is a hyperparameter we can set – basically it would be the length of the longest sentence that we can set</p><p>After embedding the words in the input sentence, each of these flow through each of the 2 layers of the encoder as shown below;</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQH9k3IjEOuEAg" src="https://media.licdn.com/dms/image/C4E12AQH9k3IjEOuEAg/article-inline_image-shrink_1000_1488/0/1629920078256?e=1691625600&amp;v=beta&amp;t=lbol48D_7gq9CUta2RdWrqkjrluclgjTsRGSvfwQOHU"></div><p><br></p><p><strong>Figure: Flow through encoder blocks</strong></p><p><br></p><p><em>One of the key properties of the Transformer is that the word in each position flows through its own path in the encoder. There are dependencies between these paths in the self-attention layer. The feed-forward layer does not have those dependencies; however, and thus the various paths can be executed in parallel while flowing through the feed-forward layer.</em></p><p>The encoder receives a list of vectors as input. It processes this list by passing these vectors into a ‘self-attention’ layer, then into a feed forward network and then sends the output upward to the next encoder</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQH8p5BY7n0wmA" src="https://media.licdn.com/dms/image/C4E12AQH8p5BY7n0wmA/article-inline_image-shrink_1000_1488/0/1629920107519?e=1691625600&amp;v=beta&amp;t=3_ZBuUd54mP3bBZuUD8WXkj_C-mq0Ykx_uMetqoWX68"></div><p><strong>Figure: The word at each position passes through a self-attention process. Then, through the feed forward network and then through the next encoder unit comprising of again the self-attention and feed forward layer and so on…</strong></p><h2>13.&nbsp;&nbsp;&nbsp;Self-Attention in detail</h2><p><strong>&nbsp;</strong></p><p><strong>Self-Attention Layer:</strong></p><p>As stated, and briefly explained in section, in the self-attention later, we ask the question: <em>“how relevant is the ith word in the English sentence to other words in the same sentence”</em>. This can be better illustrated through another example;</p><p>Considering the sentence;</p><p><strong><em>The LCF life of the bolt hole was found to be 20,000 cycles and it’s worst principal stress was ~860 MPa.</em></strong></p><p>Here, when the model is processing the word “it’s”, self-attention allows “it’s” to be associated with the bolt-hole. As the model processes each word, self-attention allows it to look at other positions in the input sentence for clues that help lead to a better encoding for this word.</p><p>The concept of attention is like the hidden states in RNN that allows an RNN to incorporate its representation of of previous words/vectors it has processed with the current one its processing. <em>Self-attention is the method the Transformer uses</em> to bake the understanding of other relevant words <em>into the one that’s currently being processed</em></p><p><br></p><p><strong>The Mathematics in Self-Attention</strong></p><p><strong>Mathematical Step 1: The Query, Key and Value Vectors</strong></p><p>The first step in calculating self-attention is to create three vectors from each of the encoder’s input vectors (in this case the embedding of each word). So, for each word, we create a;</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Query Vector</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Key Vector</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Value Vector</p><p><strong>These vectors are created by multiplying the embedding by 3 matrices that are trained during the training process.</strong></p><p><br></p><p>Notice that these new vectors are smaller in dimension than the embedding vector. Their dimensionality is 64 whereas the embedding and encoder input/output vectors have dimensionality 512. They don’t have to be smaller, but this is just an architectural choice.</p><p><br></p><p>Let us consider just 2 words in a sentence as below to illustrate this;</p><p>That is, let us consider the sentence;</p><p><strong>Thinking Machines&nbsp;</strong></p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQGp2o-R4yWL3Q" src="https://media.licdn.com/dms/image/C4E12AQGp2o-R4yWL3Q/article-inline_image-shrink_1500_2232/0/1629920225666?e=1691625600&amp;v=beta&amp;t=LT9O7m8r-F5c8LNIvvCJqOUD-GYENN-SueovkGJaroQ"></div><p><br></p><p><strong>In the above, the embedding vector (1 x 4) of each word multiplied with the W</strong>Q, WK, WV matrices results in the query, key and value vector of each word. These matrices WQ, WK, WV will be ‘learnt’ during the learning process.</p><p><strong>Mathematical Step 2: Getting the Score </strong></p><p>The second step in calculating self-attention is to calculate a score. Say, we’re calculating the self-attention for the first word in the sentence: “Thinking”, then, we need to score each word of the input sentence against this word. The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position.</p><p>The score is calculated by taking the <strong>dot product of the query vector with the key vector</strong> of the respective word we’re scoring. So, if we’re processing the self-attention for the word in position #1, the first score would be dot product of q1 and k1. The second score would be dot product of q1 and k2.</p><p><br></p><p>Note: It may be noted that;</p><p><em>The physical significance of dot product of two vectors is the just similarity of two vectors, you project one vector onto another direction of vector. Take a special case as an example, there are two vectors which have 90 degrees between them, you can get result of&nbsp;</em><strong><em>0</em></strong><em>&nbsp;when you do dot product of these two vectors, that is, the similarity of two vectors are zero.</em></p><p><em>Mathematically, dot product of 2 vectors A and B is given by;</em></p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQEWpq40qZFMwg" src="https://media.licdn.com/dms/image/C4E12AQEWpq40qZFMwg/article-inline_image-shrink_400_744/0/1629920284447?e=1691625600&amp;v=beta&amp;t=UBQ5ITxrX-sa2bZM_vpGBKTh7xjDsUtemADxK1A4J4Q"></div><p><strong>Mathematical Step 3: Division of the score by square root of the dimension of the key vector</strong></p><p>The third step in the calculation of self-attention is to divide the score by 8 (which is the square root of dimension of the key vector used in paper – 64. This is to give more stable gradients</p><p><strong>&nbsp;</strong></p><p><strong>Mathematical Step 4: Passing the result through softmax layer</strong></p><p>We then pass the result through a softmax operation, the softmax normalizes the scores so that they’re all positive and add upto 1. This is discussed in section</p><p>&nbsp;</p><p>The steps 1 through 4 are shown below;</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQHqei46NjxuBg" src="https://media.licdn.com/dms/image/C4E12AQHqei46NjxuBg/article-inline_image-shrink_1000_1488/0/1629920355081?e=1691625600&amp;v=beta&amp;t=sIgks6iEvufegBZGxxsFx2QZaJCLQmMnr7uNjPHORUA"></div><p><br></p><p><strong>Figure: Steps 1 through 4 of Self Attention Calculation</strong></p><p><br></p><p><strong>Mathematical Step 5: Multiplying the value vector by the softmax score</strong></p><p>The fifth step is to multiply each <strong>value </strong>vector by the softmax score (in preparation to sum them up). The intuition here is to keep intact the values of the word(s) we want to focus on and throw away irrelevant words (by multiplying them by tiny numbers like 0.0001 for example).</p><p><strong>Mathematical Step 6: Summing up the weighted value vectors</strong></p><p>The next step is to sum the weighted value vectors which gives the output for the self-attention layer at this position (for the first word). </p><p>The summary of the steps are illustrated below;</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQGMBPUwHNq0GQ" src="https://media.licdn.com/dms/image/C4E12AQGMBPUwHNq0GQ/article-inline_image-shrink_1000_1488/0/1629920500729?e=1691625600&amp;v=beta&amp;t=HBw1f4BrZej3EboLPDYyU2KyhAHsyXHT4bTrBu6RUy4"></div><p><strong>Figure: Illustration of mathematical steps to compute self-attention of the first word (i.e. how relevant is the first word </strong></p><p>This concludes the self-attention calculation. The resulting vector is one we can send along to the feed-forward neural network. In actual implementation, calculation of self-attention is done in a matrix form to enable fast processing and this is shown below;</p><p><strong>Matrix calculation of Self-Attention:</strong></p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQEcbiqqBErApw" src="https://media.licdn.com/dms/image/C4E12AQEcbiqqBErApw/article-inline_image-shrink_1000_1488/0/1629920592281?e=1691625600&amp;v=beta&amp;t=MpADwBg-9kZLGyVG84_XnfcKcvqVZHDk5rbw_1-Aa2Q"></div><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQElNAu3tqHMEA" src="https://media.licdn.com/dms/image/C4E12AQElNAu3tqHMEA/article-inline_image-shrink_1000_1488/0/1629920606524?e=1691625600&amp;v=beta&amp;t=eVzRvpFIwUg-o08-9inLVRF4Vs1Zk-5S3Kph_ofABYw"></div><p><br></p><p><strong>Figure: Self-Attention calculation in matrix form</strong></p><p><strong>Multi-head Attention: </strong></p><p>As mentioned before, in the Attention block, we want to answer the question: “how relevant is the ith word in the English sentence to all other words in the same sentence”. The Attention block computes the attention vectors for each word. The only problem is that for each word the attention vector may weight its relation much higher with itself – which is true, but we’re more interested in its interaction with different words. So, in the Transformer model we compute eight such attention vector per word. Since we use multiple attention vectors, we call it “<strong>Multi-headed attention”</strong></p><p>The Attention vectors are then passed onto the feed forward net one vector at a time and since each of the attention nets are independent of each other – this task can be done in parallel.&nbsp;Thus, with multi-headed attention we have not only one but multiple sets of Query | Key | Value matrices – the Transformer model uses 8 such heads as mentioned above so that we end up with 8 sets for each encoder / decoder. Each of the sets is randomly initialized at the beginning.</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQFOmgbLiXYHjw" src="https://media.licdn.com/dms/image/C4E12AQFOmgbLiXYHjw/article-inline_image-shrink_1000_1488/0/1629920892742?e=1691625600&amp;v=beta&amp;t=jtF5G2aWv2XrRbUk7n2FLlXRU_oHG_4pSW-5ivCy8y4"></div><p><br></p><p><strong>Figure: Multi-headed attention</strong></p><p>&nbsp;</p><h2>14.&nbsp;&nbsp;&nbsp;Details of the Decoder Block</h2><p><br></p><p>Now that we have covered most of the concepts of the encoder side – we know how the decoders work as well. Described below is how they work together;</p><p>The encoder starts by processing the input sentence. The output of the top encoder is transformed into a set of attention vectors K and V. Those are used by each decoder in its “encoder-decoder&nbsp;attention layer which helps the decoder focus on the appropriate places in the input sentence as shown in the figure below;</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQEx93fbkqJMxQ" src="https://media.licdn.com/dms/image/C4E12AQEx93fbkqJMxQ/article-inline_image-shrink_1000_1488/0/1629921134640?e=1691625600&amp;v=beta&amp;t=oqkiY6se-UKXnGdSR3AjLqW-RHnuAHx-Lsle5EdMcao"></div><p><br></p><p>&nbsp;<strong>Figure: Outputs from the last encoder passed to the Encoder-Decoder Attention Layer</strong></p><p><br></p><p>The following steps repeat the process until a special symbol is reached indicating the transformer decoder has completed its output. The output of each step is fed to the bottom decoder in the next time step and the decoders bubble up their decoder results just like the encoders did. And just like we did with the encoder inputs, we embed and add positional encoding to these decoder inputs to indicate the position of each word as shown below;</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQHpC2Du5kFuMA" src="https://media.licdn.com/dms/image/C4D12AQHpC2Du5kFuMA/article-inline_image-shrink_1000_1488/0/1629994114780?e=1691625600&amp;v=beta&amp;t=HU2XSn0dxjhfOfIA_kxA34E9lXi0zNme8nrYBKjLLuo"></div><p><strong>                       Figure: Outputs from each step passed to the bottom decoder</strong></p><p><br></p><p><strong>Self-Attention layers in the Decoder</strong></p><p>The Self-Attention Layers in the decoder operate in a slightly different way than the one in the encoder. In the decoder the self-attention layer can attend to earlier positions in the output sequence. This is done by masking future positions (Setting them to -inf) before the softmax step in the self-attention layer.</p><p>The “Encoder-Decoder Attention” layer works just like multiheaded self-attention, except it creates its Queries matrix from the layer below it and takes the Keys and Values matrix from the output of the encoder stack.</p><p><br></p><p><strong>The Final Linear Layer and the Softmax layer:</strong></p><p>The decoder stacks a vector of floats. How do we convert into a float? That’s the job of the Linear layer and the Softmax layer</p><p><br></p><p><strong>What is the Linear Layer?</strong></p><p>The Linear Layer is a simple <strong>fully connected neural network</strong> that projects the vector produced by the stack of decoders, into a much, much larger vector called a “logits” vector. E.g. let us say that that our model knows 10,000 unique French words (i.e. the models “output vocabulary”) that its learnt from the training dataset. This will make the “logits” vector 10,000 cells wide – each cell corresponding to the score of the unique word.&nbsp;</p><p><strong>What is the Softmax Layer?</strong></p><p>The Softmax layer turns the scores into probabilities (all positive, all adding to 1.0). The cell with the highest probability is chosen and the word associated with it is produced as the output for this time step.</p><p><strong>Recap of Training</strong></p><p>Now that we’ve covered the entire forward pass process through the trained Transformer, it will be useful to have a glance at the intuition of training a model.</p><p>During training, an untrained model would go through the exact forward pass. But since we’re training on a labelled trained model – we can compare the output with the actual correct output. To visualize this, let us assume that the output vocabulary contains six words (“a”, “am”,” I”, “think”, “student” and “&lt;eos&gt;” as below;</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQHSnPQxwxVjew" src="https://media.licdn.com/dms/image/C4D12AQHSnPQxwxVjew/article-inline_image-shrink_400_744/0/1629994228097?e=1691625600&amp;v=beta&amp;t=XvvoHSklXnLWFZ0Qi3EpAnQSu_ruW5WCqt9tT7Wfz_o"></div><p><strong>One-hot encoding</strong></p><p>Once we define the output vocabulary, we can use a vector of the same width to indicate each word in our vocabulary. This is also known as “one-hot encoding”. So, for example, we can indicate the word “am” using the following vector – this is also known as: <strong>One hot encoding</strong></p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQGsDpsCPIpIbQ" src="https://media.licdn.com/dms/image/C4D12AQGsDpsCPIpIbQ/article-inline_image-shrink_1000_1488/0/1629994253443?e=1691625600&amp;v=beta&amp;t=kSKuviyjgjivpzSOBncl9IPGe8abKalnCQCJgYkyGOU"></div><p><strong>                        Example: One hot encoding of the Output vocabulary</strong></p><p>Following this recap, let’s discuss the model’s loss function – the metric we’re optimizing during the training phase to lead up to a trained and accurate (hopefully!) model.</p><p><strong>&nbsp;</strong></p><p><strong>The Loss Function:</strong></p><p>Let us say we’re training our model. Sand we’re training it on a simple example – translating “merci” to “thanks”. What this means is that we want the output to be a probability distribution indicating the word “thanks”. But since the model is not yet trained, that’s unlikely to happen yet:</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQFEApCAMHacKw" src="https://media.licdn.com/dms/image/C4D12AQFEApCAMHacKw/article-inline_image-shrink_1000_1488/0/1629994391043?e=1691625600&amp;v=beta&amp;t=i-AbK74jiw4J_1g4nscMVVZUmQyTcjTfwvK8l7EWAaE"></div><p><strong>Figure: Output Vocabulary</strong></p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQHDgCtveDuNBg" src="https://media.licdn.com/dms/image/C4D12AQHDgCtveDuNBg/article-inline_image-shrink_400_744/0/1629994406587?e=1691625600&amp;v=beta&amp;t=DKAGDtZV3J04EDGcljyrpHdFTTz3mycFiM4jC0lxXeU"></div><p><strong>Figure: Untrained Model Output</strong></p><p><strong>&nbsp;</strong></p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQFR0nPiihClzw" src="https://media.licdn.com/dms/image/C4D12AQFR0nPiihClzw/article-inline_image-shrink_400_744/0/1629994422824?e=1691625600&amp;v=beta&amp;t=mU7gozIZ3Jm3dpnsIM4y6lj0AoGdy770jpBPyB2RWLk"></div><p><strong>Figure: Correct and Desired output</strong></p><p><strong><em>Since the model’s parameters (the weights) are all initialized randomly, the untrained model produces a probability distribution with arbitrary values for each cell/word. We can compare it with the actual output and tweak the model’s weights by using backpropagation to make the output closer to the desired output.</em></strong></p><p><br></p><p>Cross Entropy loss function is generally used, and this is not elaborated as might digress the topic </p><p>&nbsp;</p><p><strong>A slightly more realistic example</strong></p><p>It may be underscored that the above is an oversimplified example – more realistically a sentence longer than one word is generally used – for example: we input – “Je suis étudiant” and the expected out is “I am a student” – what we want is our model to successively output probabilkity distributions where;</p><p><br></p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Each probability distribution is represented by a vector of width vocab_size (6 in our toy example, but more realistically a number like 30,000 or 50,000)</p><p><br></p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The first probability distribution has the highest probability at the cell associated with the word “I”</p><p><br></p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The second probability distribution has the highest probability associated with the word “am”</p><p><br></p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;And so on, until the fifth probability distribution indicating &lt;end of sentence&gt; symbol which also has a cell associated with it from the 10,000-element vocabulary</p><p><br></p><p><strong>Target Model Outputs and Trained Model Outputs:</strong></p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQHQx9isqvM5Lg" src="https://media.licdn.com/dms/image/C4D12AQHQx9isqvM5Lg/article-inline_image-shrink_1000_1488/0/1629994545649?e=1691625600&amp;v=beta&amp;t=GTQQ_0w8-SfusVlrlkNJ_0CxmsJMtoBCBrr_l_k_gsA"></div><p><strong>Figure- These are the target probability distributions we’ll train our model against in the training example of one sample sentence</strong></p><p><strong>&nbsp;</strong></p><p>After training the model for enough time on a large enough dataset, the produced probability distributions would look like this;</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQEKGGvV4nCrBw" src="https://media.licdn.com/dms/image/C4D12AQEKGGvV4nCrBw/article-inline_image-shrink_1000_1488/0/1629994568125?e=1691625600&amp;v=beta&amp;t=stu85Mbp8T4IPjoG993LvMxWs4HPu0FCXDsGorfbmDY"></div><p><strong>Figure – Trained Model Outputs</strong> <em>– it may be noted that every position gets a a bit of probability even though it’s unlikely to be the output of the step – that’s a very useful property of softmax which helps the training process</em></p><p><br></p><p><strong>Greedy Decoding and Beam Search</strong></p><p>Now, because the model produces outputs one at a time, we can assume that the model is selecting the highest probability from that probability distribution and throwing away the rest. That’s one way to do it and it’s called: <strong>Greedy Decoding</strong>.</p><p>Another way to do it might be to hold on to the 2 top words and then in the next step run the model twice: once assuming that the first output was the first output was the word “I” and another time assuming that the first output was the word “a” and then whichever version produced les error considering both will be retained. This method is called: “<strong>beam search</strong>”</p><p><br></p><h2><strong>15.&nbsp;&nbsp;BERT - </strong>Bi-directional Encoding Representation from Transformer</h2><p><br></p><p>The Transformer Neural Network Architecture was originally created to solve the problem of language translation and that was very well received. Until this time, the LSTM had been sued to solve the problem of language translation and they had their own problem – mainly;</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LSTMs were slow to train and did not capture the true meaning of words</p><p><br></p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Even though LSTMs were bi-directional they were technically learning left to right to left context separately and then concatenating them, so the true context was slightly lost</p><p><br></p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The transformer architecture addressed some of these concerns as pointed and elaborated in the discussions above – they are faster as words can be processed simultaneously; the context of words is better learnt as they learn the context from both directions simultaneously.</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQFxxgzMMmRSpg" src="https://media.licdn.com/dms/image/C4D12AQFxxgzMMmRSpg/article-inline_image-shrink_1500_2232/0/1629994688494?e=1691625600&amp;v=beta&amp;t=rnqbAJaku2ji0Bn7EtgboSu1Phm276SpwjD7JOFrcjQ"></div><p><strong>                                         Figure: Transformer Architecture</strong></p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQE2ga_PXdQ3sg" src="https://media.licdn.com/dms/image/C4D12AQE2ga_PXdQ3sg/article-inline_image-shrink_1000_1488/0/1629995744471?e=1691625600&amp;v=beta&amp;t=cmFOu9nZ8xGNVmn_DoTwvOOtYlDUEAx7n-ckeGirAsw"></div><p><strong>                                        Figure: LSTM Networks – slow to train&nbsp;</strong></p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQHo0MWSgct8TA" src="https://media.licdn.com/dms/image/C4D12AQHo0MWSgct8TA/article-inline_image-shrink_1500_2232/0/1629996080178?e=1691625600&amp;v=beta&amp;t=MhFekcbz6L1kUXmjDsEQ0YHQZMasKTICpwwjG6Zpf6o"></div><p><strong>                                                  Figure: Bidirectional LSTM Networks </strong></p><p><strong>&nbsp;</strong></p><p><strong>&nbsp;</strong></p><p><strong>Transformer in action:</strong></p><p>As pointed above, the transformer consists of two key components: and encoder and a decoder and the working of the encoder and decoder has been elaborated above. Something that is physically appealing in the Transformer is – we see a separation of tasks in the encoder and decoder. That is: </p><p>a)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the encoder learns what is English, what is grammar and more importantly what is context (through the multiheaded attention layer)</p><p>b)&nbsp;&nbsp;&nbsp;&nbsp;The decoder learns how the English words relate to the French words&nbsp;</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQEbHQCJqZ_4kg" src="https://media.licdn.com/dms/image/C4D12AQEbHQCJqZ_4kg/article-inline_image-shrink_1500_2232/0/1629996284787?e=1691625600&amp;v=beta&amp;t=DpCEqeFkCqWbQbO0UuwVvsycX8npIeH7C55DJC5ZvRk"></div><p><strong>                                              Figure: Transformer vs LSTMs</strong></p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQHYiuvPIwyzLw" src="https://media.licdn.com/dms/image/C4D12AQHYiuvPIwyzLw/article-inline_image-shrink_1000_1488/0/1629996407672?e=1691625600&amp;v=beta&amp;t=jK-VRd4SD2finolQ08Es3z-LaPNUwSiWUzlqdOtowXI"></div><p><strong>                      Figure: Encoder and Decoder components of the Transformer</strong></p><p><strong>&nbsp;</strong></p><p><strong>GPT and BERT:</strong></p><p>We stack the decoders we get the GPT transformer architecture and conversely, we stalk the encoders and we get the BERT -Bi-directional Encoding Representation from Transformer. We can use BERT for the following tasks;</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Neural Machine Translation</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Question Answering</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sentiment Classification</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Text summarization</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;And many more tasks…</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQHEeL75vSF9sA" src="https://media.licdn.com/dms/image/C4D12AQHEeL75vSF9sA/article-inline_image-shrink_1000_1488/0/1629996629805?e=1691625600&amp;v=beta&amp;t=7b30cs5wtygNqTEc0YZsSecLPxS9SP9ZAyBKdxOViKk"></div><p><strong>                           Figure: Stack the Encoders – You get “BERT”</strong></p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQFd4uuLF6KXQQ" src="https://media.licdn.com/dms/image/C4D12AQFd4uuLF6KXQQ/article-inline_image-shrink_1500_2232/0/1629996660277?e=1691625600&amp;v=beta&amp;t=JbFyo4AnUq-8srmFTBEZr4npCE1hlpyge9l1IIG79Kg"></div><p><strong>                     Figure: Stack the Decoders – You get “GPT”</strong></p><p><strong>&nbsp;</strong></p><p><strong>Training of BERT:</strong></p><p>All these problems require understanding of language so we can train BERT to understand language and then fine tune BERT depending upon the problem we want to solve. The training of BERT is done in 2 phases:</p><p>-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;First, we pre-train BERT To understand language</p><p>-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Then, we fine tune BERT to learn specific task</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQFwpCopUqbGPQ" src="https://media.licdn.com/dms/image/C4D12AQFwpCopUqbGPQ/article-inline_image-shrink_1500_2232/0/1629996804852?e=1691625600&amp;v=beta&amp;t=kkQX3YbSRh1DOtgUmb2h4JfswUMEBnRSz6h4ah7mSQA"></div><p><strong>                                         Figure: Some problems that BERT can solve!</strong></p><p><strong>Pre-Training of BERT: Pass 1</strong></p><p>The goal of pre-training BERT is to make it understand what language is and what is context. BERT learns language by pre-training on 2 unsupervised learning tasks simultaneously and they are: Mask language modelling and next sentence prediction.&nbsp;For mask language modelling BERT Takes in a sentence with random words filled with “masks” the goal is to output these mask tokens which is something like “fill in the blanks”&nbsp;and helps BERT To understand the bi-directional context within a sentence and in case of next sentence prediction the BERT Takes in two sentences and it determines if the second sentence actually follows the first or not – this is like a binary classification problem and t helps BERT To understand the context across different sentences and using these 2 units together – masked language modelling and next sentence prediction, BERT has an understanding of language and context! And that is Pre-Training of BERT</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQFl4Bl5UKLBLg" src="https://media.licdn.com/dms/image/C4D12AQFl4Bl5UKLBLg/article-inline_image-shrink_1500_2232/0/1629996904775?e=1691625600&amp;v=beta&amp;t=8Otmlg83aX-MebBZZI3_XCetSKLm8qOOXMHJ11zco-k"></div><p><strong>                                                         Figure: BERT Pre-Training</strong></p><p><br></p><p><strong>Fine-tuning phase of BERT: Pass 1</strong></p><p>In the fine-tuning phase of BERT, we further train BERT on very specific NLP tasks e.g. let us take question answering – all we have to do is to replace the fully connected output layers of the network with a fresh set of output layers so that we can output the answer to the question we want. We can perform a supervised learning using a question answering data set – this learning won’t take long since its only the output parameters that are learned from the scratch and the rest of the model parameters are just fine tuned and as a result the training time is fast and we can do this for any Natural language processing problem i.e. replace the output layers and then train it with specific data</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQECsqheC8DWsw" src="https://media.licdn.com/dms/image/C4D12AQECsqheC8DWsw/article-inline_image-shrink_1500_2232/0/1629997044459?e=1691625600&amp;v=beta&amp;t=7Q3AsKO8IutdhuYfXE831CcHeFncQBmJpyNI4s4KF3o"></div><p><strong>                                           Figure: BERT – Fine Tuning</strong></p><p><strong>&nbsp;</strong></p><p><strong>More details on Pre-Training: Pass 2</strong></p><p>During BERT Pre-training we train the model on: Mask Language Modelling and Next Sentence Prediction – in practise both problems are trained simultaneously. The input here is a set of 2 sentences with some of the words being masked – each token is a word. On the output side “c” is the binary output for the next sentence prediction – so it would output 1 if the sentence B follows the sentence A in context and 0 if sentence B does not follow the sentence A.</p><p>Each of the T’s here are word vectors that correspond to the output of the language model problem so that the number of word vectors that we input are the same as the number of vectors that we output</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQGzrXb7uhCjiQ" src="https://media.licdn.com/dms/image/C4D12AQGzrXb7uhCjiQ/article-inline_image-shrink_1500_2232/0/1629997132139?e=1691625600&amp;v=beta&amp;t=VrN52skUrMqtRNluVTuyBfZEBbzPaJv5rTR1hn8S2nw"></div><p><br></p><p><strong>                        Figure: Pre-Training – Pass 2</strong></p><p><strong>&nbsp;</strong></p><p><strong>More on the Fine-Tuning Phase: Pass 2</strong></p><p>On the Fine tuning phase, if we wanted to perform question answering, we would train the model by modifying the input and output layer – in the input layer we pass in the question followed by the passage containing the answer and in the output layer we output the start N words that encapsulate the answer assuming that the answer is within the same span of the text – as illustrated in the figure</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQH2TuZWZPArdw" src="https://media.licdn.com/dms/image/C4D12AQH2TuZWZPArdw/article-inline_image-shrink_1500_2232/0/1629997222515?e=1691625600&amp;v=beta&amp;t=2o9qXpucy7WWu3VE0om8G2G1KJKkakEKFmOOrLot2MQ"></div><p><strong>                               Figure: Fine Tuning – Pass 2</strong></p><p>&nbsp;</p><p><strong>&nbsp;More details on Pre-Training: Pass 3</strong></p><p>On the input side – how are we going to generate input embeddings from the word token inputs? The initial embeddings are constructed from 3 vectors: Token embeddings, segment embeddings and position embeddings. The initial paper uses word piece embeddings that have a vocabulary of 30,000 tokens, the segment embeddings is actually a sentence number that is encoded into a vector and the position embedding Is the position of a word within the sentence that is encoded into a vector. Adding these 3 vectors together we get an embedding vector that uses an input to BERT The Segment and position embeddings are added for add “ordering” for inputs.</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQGF-BCWuGDwGg" src="https://media.licdn.com/dms/image/C4D12AQGF-BCWuGDwGg/article-inline_image-shrink_1500_2232/0/1629997291552?e=1691625600&amp;v=beta&amp;t=4PuuApYPlvO4Upz51y0caA9FFn2DrBhRtamYaUoAeKI"></div><p><strong>                         Figure: Pre-Training – Pass 3</strong></p><p><br></p><p>The output is a binary value “C” and a bunch of word vectors but with training we need to minimize a loss. Two key details to note here:</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The Word Vectors Ti have the same size</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Word Vectors Ti are generated simultaneously</p><p>We need to take each word vector, pas it to the fully connected layered output equal to the same number of neurons equal to the number of tokens in the vocabulary and apply a softmax activation. This way we would convert a word vector to a distribution, and the actual label of the distribution would be a one hot encoded vector for the actual word and so we compare the two distributions and train the network using the cross-entropy loss. Note that the output has all the words (even the words that weren’t masked at all) but the loss considers the prediction of the masked word and ignores all other word that are output by the network.</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4D12AQE_GhVT57uJkg" src="https://media.licdn.com/dms/image/C4D12AQE_GhVT57uJkg/article-inline_image-shrink_1500_2232/0/1629997360435?e=1691625600&amp;v=beta&amp;t=e1oB6n1MbybMmgZjY3fwKsh6xodWozumcEAQ6XM8HfY"></div><p><strong>                                                 Figure: Pre-Training – Pass 3 - Output</strong></p><p><br></p><p><br></p><p><br></p><p><br></p><p><br></p><p><br></p><p><br></p><p>&nbsp;&nbsp;</p><p><br></p><p><br></p><p><br></p><p><br></p><p><br></p><p><br></p><p><br></p><p><br></p><p><br></p><p><br></p><p><br></p><p><br></p><p><br></p><p><br></p><p><br></p><p><br></p><p><br></p><p><br></p><p><br></p><p><br></p><p><br></p><p><br></p><p><br></p></div>
</body>
</html>