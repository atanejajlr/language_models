<html>
<head>
  <title>ChatGPT and how it works – my notes: Part 1</title>
  <style>
    body {
      margin: 0 auto;
      width: 744px;
      font-family: Source Serif Pro, serif;
      line-height: 32px;
      font-weight: 400;
      color: rgba(0, 0, 0, 0.7);
      font-size: 21px;
    }
    h1, h2, h3 {
      font-family: Source Sans Pro, Helvetica, Arial, sans-serif;
    }
    h1 a, h1 a:visited {
      color: inherit;
      text-decoration: none;
    }
    h1 {
      line-height: 48px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 42px;
      margin: 32px 0 20px;
    }
    h2 {
      line-height: 32px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 26px;
      margin: 28px 0;
    }
    h3 {
      line-height: 28px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 21px;
      margin: 24px 0;
    }
    p {
      margin: 32px 0;
    }
    .created, .published {
      color: rgba(0, 0, 0, 0.55);
      font-size: 15px;
      line-height: 15px;
      margin: 20px 0;
    }
    .created + .published {
      margin-top: -12px;
    }
    blockquote {
      font-family: Georgia, Source Serif Pro, serif;
      font-style: italic;
      font-size: 24px;
      line-height: 36px;
      margin: 48px 120px;
      text-align: center;
    }
    a {
      word-wrap: break-word;
      outline: none;
      text-decoration: none;
      background-color: transparent;
      border: 0;
      color: #008CC9;
    }
    a:hover {
      text-decoration: underline;
    }
    a:visited {
      color: #8C68CB;
    }
    .center {
      text-align: center;
    }
    iframe {
      display: block;
      margin: 44px auto;
    }
    *:not(pre) + pre, pre:first-of-type {
      margin-top: 32px;
      padding-top: 32px;
    }
    pre:only-of-type {
      margin: 32px 0;
      padding: 32px;
    }
    pre {
      background: #F3F6F8;
      overflow-x: auto;
      display: block;
      font-size: 13px;
      font-family: monospace;
      line-height: 13px;
      padding: 0 32px 32px;
      white-space: pre;
    }
    a.embedded {
      background: #F3F6F8;
      display: block;
      padding: 32px;
      margin: 32px 0;
    }
    img {
      height: auto;
      max-width: 100%;
    }
    .slate-image-embed__resize-full-width img {
      width: 100%;
    }
    .series-logo {
      width: 48px;
      height: 48px;
      box-sizing: border-box;
      background-clip: content-box;
      border: 4px solid transparent;
      border-radius: 6px;
      object-fit: scale-down;
      float: left;
    }
    .series-title {
      font-size: 16px;
      font-weight: 600;
      vertical-align: top;
    }
    .series-description {
      color: rgba(0,0,0,.6);
      font-weight: 400;
      font-size: 14px;
      line-height: 20px;
    }
    div {
      margin: 32px 0;
    }
  </style>
</head>
<body>
    <img src="https://media.licdn.com/mediaD4E12AQHeT-0u36yAXg" alt="" title="" />
      <h1><a href="https://www.linkedin.com/pulse/chatgpt-how-works-my-notes-part-1-ajay-taneja">ChatGPT and how it works – my notes: Part 1</a></h1>
    <p class="created">Created on 2017-09-03 16:59</p>
  <p class="published">Published on 2023-02-25 23:35</p>
  <div><p>𝐼 𝑤𝑖𝑙𝑙 𝑏𝑒 𝑝𝑢𝑏𝑙𝑖𝑠ℎ𝑖𝑛𝑔 𝑎 𝑠𝑒𝑟𝑖𝑒𝑠 𝑜𝑓 𝑎𝑟𝑡𝑖𝑐𝑙𝑒𝑠/𝑏𝑙𝑜𝑔𝑠 𝑜𝑛 𝑡ℎ𝑒 𝑠𝑡𝑒𝑝𝑠 𝑖𝑛𝑣𝑜𝑙𝑣𝑒𝑑 𝑖𝑛 𝐶ℎ𝑎𝑡𝐺𝑃𝑇 𝑡𝑟𝑎𝑖𝑛𝑖𝑛𝑔. 𝑇ℎ𝑖𝑠 𝑖𝑠 𝑡ℎ𝑒 𝑓𝑖𝑟𝑠𝑡 𝑏𝑙𝑜𝑔 / 𝑝𝑜𝑠𝑡 𝑖𝑛 𝑡ℎ𝑒 𝑠𝑒𝑟𝑖𝑒𝑠</p><h2>1.&nbsp;&nbsp;&nbsp;Introduction</h2><p><strong>What is ChatGPT?</strong></p><p>ChatGPT is a language model that takes a user prompt and generates a textual response. The responses generated by ChatGPT are the most realistic of any chatbot existing in the world until today. Understanding ChatGPT requires the understanding of the following concepts:</p><ul><li>The GPT – that is – the Generative Pre-trained transformers which are the language models built on top of the Transformer Neural Network</li><li>&nbsp;Reinforcement Learning&nbsp;</li></ul><p><br></p><figure class="slate-resizable-image-embed slate-image-embed__resize-full-width"><div></div><div></div><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4E12AQFjqnbpXztPFg" src="https://media.licdn.com/dms/image/D4E12AQFjqnbpXztPFg/article-inline_image-shrink_1000_1488/0/1677367298860?e=1691625600&amp;v=beta&amp;t=k6e7Rda0O0DWTPrEA_PduhdHq0q_bl2pxCLhHQCqQcg"></figure><p>Let us understand the above ingredients of ChatGPT in some detail:</p><p>Firstly, </p><p><strong>Language Models – What are Language Models?</strong></p><p>Language models are models which have an inherent understanding of language in a mathematical sense. Language models understand the probability distribution of a sequence of words. That is: given a context of words that have preceded it, language models will know what word (or, precisely the token) to generate next. One can get language models to handle specific tasks like:</p><ul><li>Question and Answering</li><li>Text Summarization</li><li>Machine Translation</li><li>More.</li></ul><p>Some of the language models include the following: </p><p>1.&nbsp;&nbsp;&nbsp;Bidirectional Encoding Representation of Transformers [BERT]</p><p>2.&nbsp;&nbsp;&nbsp;Generative Pretrained Transformers [GPT]</p><p>3.&nbsp;&nbsp;&nbsp;Recurrent Neural Networks [RNN]</p><p>4.&nbsp;&nbsp;&nbsp;Long Short-Term Memory [LSTM] networks</p><p>The Language models are pre-trained on general language data and then fine-tuned depending upon the task we want to solve. ChatGPT is a Generative Pretrained Transformer (GPT) model that is fine tuned to respond to a user’s request and then fine tuned further by reinforcement learning. I will discuss about reinforcement learning in a subsequent post.&nbsp;</p><h2>2.&nbsp;&nbsp;&nbsp;How is ChatGPT trained?</h2><p>Let us go a bit deeper into ChatGPT. ChatGPT has been trained using Reinforcement Learning from Human Feedback (RLHF). The training of ChatGPT can be summarized into 3 basic steps:</p><p><strong>Step 1:</strong> </p><p>An initial model of ChatGPT was trained using supervised fine tuning (SFT) – in which there was already a pre-trained language model – the pre-trained language model in case of ChatGPT was Generative Pre-Trained Transformer (GPT).</p><p>Fine tuning of GPT has been carried out using human AI trainers that provided the questions and answers. That is: the AI trainers – the labellers write a prompt and also write the response to the answer of the prompts – this is the supervised fine tuning of the GPT model.</p><p><strong>Step 2:</strong></p><p>In the next step of ChatGPT training, a comparison data was collected which consisted of 2 or more “model” responses ranked by quality. In this case, the responses were that of the fine tuned GPT model but were ranked by human AI trainers. </p><p>This prompt / response / ranking is used to train another GPT model which is called the “Rewards Model”. The input to the “Rewards Model” is the prompt and one of the responses by the supervised fine tined model of Step 1 and the output is going to be a reward which is the quantification of how good the response was.&nbsp;</p><p><strong>Step 3:</strong></p><p>In the step 3 of the GPT training, an unseen prompt is taken and passed through a copy of the supervised fine-tuned model and then through the rewards model of step 2 to get a rank. </p><p>Whatever rank is obtained is used to further fie tune the model to create a better response. This process helps the GPT model to create more factual responses, more coherent responses with hat of the question and eliminating any biased or adverse responses.</p><figure class="slate-resizable-image-embed slate-image-embed__resize-full-width"><div></div><div></div><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4E12AQFqtInbeO0neA" src="https://media.licdn.com/dms/image/D4E12AQFqtInbeO0neA/article-inline_image-shrink_1000_1488/0/1677367427980?e=1691625600&amp;v=beta&amp;t=yp8H4iiSRv0fghe5rDWpAK9ar4vEi7fn4m4TPuoYWIw"></figure><p><br></p></div>
</body>
</html>