<html>
<head>
  <title>Revisiting Support Vector Machines</title>
  <style>
    body {
      margin: 0 auto;
      width: 744px;
      font-family: Source Serif Pro, serif;
      line-height: 32px;
      font-weight: 400;
      color: rgba(0, 0, 0, 0.7);
      font-size: 21px;
    }
    h1, h2, h3 {
      font-family: Source Sans Pro, Helvetica, Arial, sans-serif;
    }
    h1 a, h1 a:visited {
      color: inherit;
      text-decoration: none;
    }
    h1 {
      line-height: 48px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 42px;
      margin: 32px 0 20px;
    }
    h2 {
      line-height: 32px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 26px;
      margin: 28px 0;
    }
    h3 {
      line-height: 28px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 21px;
      margin: 24px 0;
    }
    p {
      margin: 32px 0;
    }
    .created, .published {
      color: rgba(0, 0, 0, 0.55);
      font-size: 15px;
      line-height: 15px;
      margin: 20px 0;
    }
    .created + .published {
      margin-top: -12px;
    }
    blockquote {
      font-family: Georgia, Source Serif Pro, serif;
      font-style: italic;
      font-size: 24px;
      line-height: 36px;
      margin: 48px 120px;
      text-align: center;
    }
    a {
      word-wrap: break-word;
      outline: none;
      text-decoration: none;
      background-color: transparent;
      border: 0;
      color: #008CC9;
    }
    a:hover {
      text-decoration: underline;
    }
    a:visited {
      color: #8C68CB;
    }
    .center {
      text-align: center;
    }
    iframe {
      display: block;
      margin: 44px auto;
    }
    *:not(pre) + pre, pre:first-of-type {
      margin-top: 32px;
      padding-top: 32px;
    }
    pre:only-of-type {
      margin: 32px 0;
      padding: 32px;
    }
    pre {
      background: #F3F6F8;
      overflow-x: auto;
      display: block;
      font-size: 13px;
      font-family: monospace;
      line-height: 13px;
      padding: 0 32px 32px;
      white-space: pre;
    }
    a.embedded {
      background: #F3F6F8;
      display: block;
      padding: 32px;
      margin: 32px 0;
    }
    img {
      height: auto;
      max-width: 100%;
    }
    .slate-image-embed__resize-full-width img {
      width: 100%;
    }
    .series-logo {
      width: 48px;
      height: 48px;
      box-sizing: border-box;
      background-clip: content-box;
      border: 4px solid transparent;
      border-radius: 6px;
      object-fit: scale-down;
      float: left;
    }
    .series-title {
      font-size: 16px;
      font-weight: 600;
      vertical-align: top;
    }
    .series-description {
      color: rgba(0,0,0,.6);
      font-weight: 400;
      font-size: 14px;
      line-height: 20px;
    }
    div {
      margin: 32px 0;
    }
  </style>
</head>
<body>
    <img src="https://media.licdn.com/mediaC4E12AQHZ7TKK77LkhQ" alt="" title="" />
      <h1><a href="https://www.linkedin.com/pulse/revisiting-support-vector-machines-ajay-taneja">Revisiting Support Vector Machines</a></h1>
    <p class="created">Created on 2020-10-26 07:57</p>
  <p class="published">Published on 2022-05-19 18:18</p>
  <div><p>This article goes into the details of the Support Vector Machines (SVM) which are a set of supervised learning methods used for classification and regression problems. This artcle is organized as follows:</p><p>We will firstly go into the introductory content related to SVMs and get an insight into the following</p><p>o&nbsp;&nbsp;What are Support Vector Machines?</p><p>o&nbsp;&nbsp;What are support vectors?</p><p>o&nbsp;&nbsp;What is the SVM Algorithm optimizing?</p><p>And then into the following aspects of Support Vector Machines (SVMs):</p><ul><li>How is the SVM algorithm different from Logistic Regression from the perspective of the cost function?</li><li>The Kernel Trick in SVMs</li><li>The Gaussian Kernel and other types of Kernels</li><li>&nbsp;How does the “C” parameter in SVM affect overfitting | treat outliers in the datase</li><li>Some tutorials in SVM</li></ul><p>&nbsp;</p><h2><strong>1. Introduction: </strong></h2><p>Let us say that you as a Machine Learning (ML) Engineer has to solve a classification problem – which ML algorithm would you first select? Most of the Engineers would first opt for the Support Vector Machines (SVM) even though they might not understand about the background and the math that goes on behind the SVMs. Let us try and understand / revisit some of the highlight ingredients that make SVM a very versatile algorithm.</p><p><strong><u>What are Support Vector Machines? </u></strong></p><p>Support Vector Machine is a type of Machine Learning Algorithm that can be used for regression and classification tasks – in general they have a wider application in classification problems than regression problems. The SVMs builds upon the logistic regression and adds a sturdier cost function (this will be detailed in a subsequent post) than the cost function in logistic regression together with the technique of kernel (more commonly referred to as <em>kernel trick</em> that makes SVMs a very versatile algorithm.&nbsp;&nbsp;SVMs can be used in a variety of problems such as: anomaly detection, text classification, image classification, handwriting detection etc.</p><p>&nbsp;<strong><u>What are Support Vectors?</u></strong></p><p>Support Vectors can most sophistically be defined as the coordinates of individual observations as shown in the figure below, the goal of the Support Machine classifier is to “learn” a boundary that best segregates these observations. The boundary is often termed as the “hyperplane” – in a 2-dimensional space, the hyperplane is (n-1) dimension separator – son a 2D space, the hyperplane is a one-dimensional line and in a 3D space, the hyperplane is a 2D plane.</p><p>The observations will depend upon the type of problem in hand – for example, one may be solving a problem to identify the presence of cancer in women depending upon certain features as: age, pregnancy within a 5-year period and family heredity of cancer, etc.</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C5612AQF2PzHW3yKAyQ" src="https://media.licdn.com/dms/image/C5612AQF2PzHW3yKAyQ/article-inline_image-shrink_1000_1488/0/1652391997862?e=1691625600&amp;v=beta&amp;t=3vu6qjl-eZeyCDfrNEr55SIxj1bkGvlgCDt0O9e6abc"></div><p><strong><u>What are Support Vector Machines optimising?</u></strong></p><p>Support Vector Machine – as a large margin classifier</p><p>Consider again a 2D case where the data is perfectly separatable i.e., we have at least one hyperplane that can separate the training data. It may be noticed from the figure below that there can be more than one hyperplane that separates the data – let us term it as positive and negative examples. The point arises, which hyperplane you would select?</p><p>We would select the hyperplane at a position where the distance from the closest points is maximum – this way we will make less mistakes with the serving data and that what the SVM does – thus, the goal of the Support Vector machine algorithm is: “To find the hyperplane which has the maximum margin from the closest point”</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C5612AQFXFHoh0qQ5xA" src="https://media.licdn.com/dms/image/C5612AQFXFHoh0qQ5xA/article-inline_image-shrink_1000_1488/0/1652392051951?e=1691625600&amp;v=beta&amp;t=qfBSDrHVcJ6x87RQ8mm73Hfg112O0ApHuAjcWVX9YtU"></div><p><strong>                             Figure: Possible hyperplanes seprating the observations</strong></p><p>Thus, from the above paragraphs we can conclude that the support vectors (that is: the set of observations) closest to the hyperplane have a direct bearing on the position of the hyperplane. These support vectors “maximize” the margin around the separating hyperplane as shown in the figure below:</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C5612AQFjjIIFV2IS0A" src="https://media.licdn.com/dms/image/C5612AQFjjIIFV2IS0A/article-inline_image-shrink_1000_1488/0/1652392096097?e=1691625600&amp;v=beta&amp;t=QnJbkvFFxPc22oTe9xA3EQMbNeXBLJGGOZwAKXnoe5Y"></div><p><strong>Inputs and outputs of SVMs vs Inputs and outputs of Neural Nets:</strong></p><p>Both – the SVM and neural nets will input the sample features x1, x2,x3..,xn and output the result y – point to be noted is that for SVMs we have just a “few” weights that correspond to the features near the decision boundary that matter in the optimization problem (unlike in the neural nets where all the weights influence the optimization) – it can eb proved mathematically that for a linear separable case, the optimization problem that we are solving in SVMs is a quadratic programming problem – hence the&nbsp;there will be only single global minima – thus we will not face any local minima making things simple!</p><p>And in case of a nonlinear decision boundary we can transform the problem into another space making it mathematically simpler – and that is where the “kernel trick” comes into play&nbsp;</p><h2>2. <strong>How Support Vector Machines (SVM) is different from Logistic Regression from the perspective of Cost Function?</strong></h2><p>The purpose of this section is to relate the cost function in Logistic Regression with the cost function in Support Vector Machines (SVM). Although the section might appear too mathematical at a glance, but this would make the overall understanding of the algorithm intuitive and help understanding how one can control overfitting when using SVMs and partly what makes the Support Vector Machines – “large margin classifier”.</p><p>&nbsp;<strong>Logistic Regression – Hypothesis and Cost Function</strong></p><p><strong>Hypothesis:</strong></p><p>Logistic Regression is used for classification in Machine Learning. Logistic Regression outputs the probability between 0 and 100%. Since the sigmoidal function occupies the range between 0 and 1, and it asymptotes to both the values – it makes it very handy form of hypothesis for binary classification problems.</p><p>Thus, for Logistic Regression,</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQFrhOsAbDY4tA" src="https://media.licdn.com/dms/image/C4E12AQFrhOsAbDY4tA/article-inline_image-shrink_400_744/0/1652942650821?e=1691625600&amp;v=beta&amp;t=qKihGHBd_HXUoMiYdP__0A25NBgqXDqibGDxqvfHdkk"></div><p>Above, the function hθ(x) maps any real number x between 0 and 1 thus making suitable for transforming any arbitrary value function into a function better suited for classification.</p><p>&nbsp;</p><p><strong>Cost Function:</strong></p><p>The cost function in Logistic Regression is represented by:</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQFUb2XI3IRAog" src="https://media.licdn.com/dms/image/C4E12AQFUb2XI3IRAog/article-inline_image-shrink_400_744/0/1652942741929?e=1691625600&amp;v=beta&amp;t=hly9-ZKSy7WAQED5AA0MuSpnwU1xYHsnqMHAH_megoQ"></div><p>This makes sense physically because if y=0 and if we’re predicting it as 1 i.e., hθ(x) approaches 1 (that is:&nbsp;our hypothesis is off from y) – then the cost approaches infinity as shown in the figure below:</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQHmykS7SQ3tzA" src="https://media.licdn.com/dms/image/C4E12AQHmykS7SQ3tzA/article-inline_image-shrink_1000_1488/0/1652942798256?e=1691625600&amp;v=beta&amp;t=wrvoEjpYiVoFiAq3s6PhVrmBV8pIAbm5_IKs8vpYL4Y"></div><p><br></p><p>And similarly, if y = 1 and the hypothesis equals 0 i.e., we’re predicting it as 0, the cost again tends to infinity as shown in the figure below:</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQEta8NLgFC79Q" src="https://media.licdn.com/dms/image/C4E12AQEta8NLgFC79Q/article-inline_image-shrink_400_744/0/1652943007642?e=1691625600&amp;v=beta&amp;t=MMkZ2r-Czgu6Vf9a_dPfoTrevz_NhGpXW7LsRn1vVo0"></div><p>From the above math, one can conclude:</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQEnsTTsNeKVIA" src="https://media.licdn.com/dms/image/C4E12AQEnsTTsNeKVIA/article-inline_image-shrink_400_744/0/1652943091402?e=1691625600&amp;v=beta&amp;t=tjkGJjC3CvccPTVJdZPHHSk4WN56-wm5MrefM-s-NrE"></div><p>Thus, the complete cost function in case of logistic regression is given by:</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQE0IxAxx44WDA" src="https://media.licdn.com/dms/image/C4E12AQE0IxAxx44WDA/article-inline_image-shrink_400_744/0/1652943137788?e=1691625600&amp;v=beta&amp;t=WPAwbHdkpmD4t-H_cKsu7h6Vs5tjiwbkHYgnna-Vr8E"></div><p><br></p><p><strong>Cost Function in Support Vector Machines:</strong></p><p>To make the Support Vector Machine (SVM), we modify the cost function of the Logic Regression as described below.</p><p>We modify the first term of the logistic regression cost function:</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQF03JdS8sHidQ" src="https://media.licdn.com/dms/image/C4E12AQF03JdS8sHidQ/article-inline_image-shrink_400_744/0/1652981872453?e=1691625600&amp;v=beta&amp;t=xuWVrLJm1sPceSQc1u7R64S5x-XgU9TcanQnoa_GcZA"></div><p>So that:</p><p>&nbsp;when <strong>θTx &gt; 1</strong> it outputs 0 and for values of z less than 1, we shall use a straight decreasing line instead of the sigmoid curve as shown below:</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQHQdU0gyH1d0A" src="https://media.licdn.com/dms/image/C4E12AQHQdU0gyH1d0A/article-inline_image-shrink_400_744/0/1652982006860?e=1691625600&amp;v=beta&amp;t=vnRVSGooSOoxVaw9MRbg5jzLRqbsF80d2oNKKUGYL0Q"></div><p>Similarly, we modify the second term of the cost function:</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQE3DziRKBHi2A" src="https://media.licdn.com/dms/image/C4E12AQE3DziRKBHi2A/article-inline_image-shrink_400_744/0/1652982082576?e=1691625600&amp;v=beta&amp;t=9UUvXVehimtkfztYI4k6zr20MgIWerC5oOongqJwsrs"></div><p>&nbsp;So that when<strong> θTx &lt; -1</strong>, it outputs 0 and again for values of z greater than -1, we use a straight increasing line instead of the sigmoid curve.</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQHtBY1_BI1Iww" src="https://media.licdn.com/dms/image/C4E12AQHtBY1_BI1Iww/article-inline_image-shrink_400_744/0/1652982213680?e=1691625600&amp;v=beta&amp;t=ZD3jk_srnk12153hTv89ndPxEaBfBYZBQSwL5kIXILY"></div><p>Now, the above cost function – a combination of the 2 terms can be casted into a suitable form of equation and now with the regularization term is written as:</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4E12AQGpK7-OzT18Bg" src="https://media.licdn.com/dms/image/D4E12AQGpK7-OzT18Bg/article-inline_image-shrink_400_744/0/1652982308077?e=1691625600&amp;v=beta&amp;t=fCXanxKJ_fBoaR5G9KqXNdDGcNGqO_UYZto8HyUQcCc"></div><p><strong>How does the above modification in the cost function (compared to that of the cost function in Logistic Regression) help SVM?</strong></p><p>&nbsp;A useful way of thinking of Support Vector Machines is to think of them as "<strong>Large Margin classifiers</strong>" as discussed in section because mathematically, we want</p><p><strong>Θ^Tx ≥ 1</strong> (not just 0 as in logistic regression) if y = 1 And</p><p><strong>Θ^Tx &lt; - 1</strong> (not just &lt; 0 as in logistic regression) if y = 0</p><p>&nbsp;</p><p>&nbsp;This helps in assuring SVM as a large margin classifier and adds an additional safety margin factor ensuring that the observations have sufficient margin from the decision boundary – it is for this reason that SVMs are termed as “Large Margin Classifiers”</p><h2>3. The Parameter "C" in Support Vector Machines</h2><p>The consequence of the optimization problem discussed in section 2 will result in a decision boundary which is at a sufficient margin from the observations as shown for the example in the below:</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQEYmzCBf4bo1A" src="https://media.licdn.com/dms/image/C4E12AQEYmzCBf4bo1A/article-inline_image-shrink_1000_1488/0/1653159638775?e=1691625600&amp;v=beta&amp;t=F2MKrAtwI82M-o_TgArKPO2Sl6OiJxWm7chhXlwbINU"></div><p><strong>    Figure: Decision boundary selected by SVM (black colour) as an advent of large margin [note: this figure is from Andrew Ngs lecture videos found of the Machine Learning course]</strong></p><p>This gives the SVM a certain robustness because it tries to separate the classes of observations with a larger margin as possible.</p><p>&nbsp;</p><p><strong>Sensitivity of the large margin classifier to outliers</strong></p><p>SVM is more sophisticated than its property of large margin classifier discussed so far. The large margin classifier can get more sensitive to outliers – if an additional observation is added for the example below, the resulting decision boundary following the minimization of the cost function will be as shown:</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQF6h_-KSLjkIA" src="https://media.licdn.com/dms/image/C4E12AQF6h_-KSLjkIA/article-inline_image-shrink_400_744/0/1653159796642?e=1691625600&amp;v=beta&amp;t=4s3nqWPDPs4htXfs2Yty6HtVa6nqCwjEoGzvmbQrfdA"></div><p><strong>                Figure: SVM Decision boundary for an example without any outliers</strong></p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQEyJL_XWlCA6A" src="https://media.licdn.com/dms/image/C4E12AQEyJL_XWlCA6A/article-inline_image-shrink_400_744/0/1653159819523?e=1691625600&amp;v=beta&amp;t=b2LMkrzzeIq1uo6ozm9wQF93z4OsnG0uiIw4-xLlK34"></div><p><strong>           Figure: SVM Decision Boundary as a consequence of outliers in the dataset</strong></p><p>However, as a Machine Learning Engineer, one might want to ignore the outlier(s) and retain the black decision boundary as shown in the figure above. This can be done through the C parameter which is a consequence of the regularization term introduced in the cost function – the mathematics of this is described below.</p><p><strong>Introducing the Regularization term in the SVM Cost Function:</strong></p><p>As stated in the section 2 above, the cost function of SVM including the regularization term is given by,</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQFlSVToT7sWtw" src="https://media.licdn.com/dms/image/C4E12AQFlSVToT7sWtw/article-inline_image-shrink_400_744/0/1653159925950?e=1691625600&amp;v=beta&amp;t=Qf3y2s0lRHmiSbFYZNN6If4MY28CLOGadB9m4aIqE_U"></div><p>Removing the m factor, we get,</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQF2Htg69rF-IA" src="https://media.licdn.com/dms/image/C4E12AQF2Htg69rF-IA/article-inline_image-shrink_400_744/0/1653159983408?e=1691625600&amp;v=beta&amp;t=M3RZkvyqg0keZsA5l4lewd5pgCPu8RHvMd-0knYLz1s"></div><p>Re-organizing and using a factor of C (C = 1/λ) , we get;</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQHlDD4S_i6MDg" src="https://media.licdn.com/dms/image/C4E12AQHlDD4S_i6MDg/article-inline_image-shrink_400_744/0/1653160017787?e=1691625600&amp;v=beta&amp;t=ugamZ1Hz9imnAmPpn1yHQHc-cY9KDVXaqd7K-MopKHQ"></div><p><strong>How does the C parameter control the decision boundary?</strong></p><p>The “C” parameter described above controls the Decision boundary. If the “C” value is very large such as: C = 100,000 then the outlier is not ignored resulting in the pink decision boundary (in the example shown) and if the “C” value is relatively small the outlier is ignored resulting in the black decision boundary. </p><p>Thus, increasing and decreasing C is similar to respectively decreasing and increasing λ, and can simplify our decision boundary.</p><h2>4. More intuition on why SVM is classified as a large margin classifier</h2><p>The hypothesis of Support Vector Machines outputs either 1 or 0 based on the evaluation of θTx i.e.</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQEN4g9bZL-MNw" src="https://media.licdn.com/dms/image/C4E12AQEN4g9bZL-MNw/article-inline_image-shrink_400_744/0/1653243284957?e=1691625600&amp;v=beta&amp;t=RkIkZpY2YUoCXv3KIsuLMJRk47hy7MJAOi6LSv8RxbU"></div><p>Considering θ and x as two dimensional for the purpose of intuition, and taking the inner product of each column (vector) θ and x. For the purpose of intuition let us represent this geometrically as described below,&nbsp;</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQEA3u21IcF9kg" src="https://media.licdn.com/dms/image/C4E12AQEA3u21IcF9kg/article-inline_image-shrink_1000_1488/0/1653243455262?e=1691625600&amp;v=beta&amp;t=lwcs8NLQxaTBvSNQiIkSfI1H9jNLNz_o628VkGNp-6w"></div><p><strong>   Figure: Considering the parameter vector theta and xth training example - a 2     dimensional case for simplification  </strong></p><p>θTx can be represented as (considering the theta and x vector as shown below):</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQGBHZ0PQ60n8Q" src="https://media.licdn.com/dms/image/C4E12AQGBHZ0PQ60n8Q/article-inline_image-shrink_400_744/0/1653243550431?e=1691625600&amp;v=beta&amp;t=X4uxAQgXIn8SUDPFr6oDo4m2R3kgc__07BucSU8f_rA"></div><p>We should also satisfy the following constraints in SVM:</p><p>&nbsp;Θ^Tx ≥ 1 (not just 0 as in logistic regression) if y = 1 And</p><p>Θ^Tx &lt; - 1 (not just &lt; 0 as in logistic regression) if y = 0</p><p>Now keeping the above in mind, if we have a set of observations as below and the SVM chooses the decision boundary as shown (green colour), then, this is not respected by the constraint:</p><p>Θ^Tx ≥ 1 (not just 0 as in logistic regression) if y = 1 And</p><p>Θ^Tx &lt; - 1 (not just &lt; 0 as in logistic regression) if y = 0</p><p>&nbsp;Because here the length of the projection being too small (for the green <em>slanting</em> decision boundary) we will not be able to satisfy Θ^Tx ≥ 1 or Θ^Tx &lt; - 1 as shown in the accompanying figure.</p><p>To satisfy the constraint, SVM will have to select the decision boundary as straight one (in this case) which gives a significant margin from the observations and hence satisfy:</p><p>Θ^Tx ≥ 1 And</p><p>Θ^Tx &lt; - 1&nbsp;</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:C4E12AQHoZq4DrvP3eA" src="https://media.licdn.com/dms/image/C4E12AQHoZq4DrvP3eA/article-inline_image-shrink_1000_1488/0/1653243625622?e=1691625600&amp;v=beta&amp;t=zGaJoTgW2gEo9UXvV6KZTPuRpM0T96I_q95kDzeCkoY"></div><p>                        <strong> Figure: How SVM chooses the correct Decision boundary</strong></p><p><br></p><h2>5.&nbsp;&nbsp;&nbsp;How does SVM handle Non-Linear Decision Boundary?</h2><p>&nbsp;</p><p>So far, in the discussion of SVM, we had a linear discussion boundary between 2 sets of observations in a dataset – we elaborated on the optimization problem subject to the constraints in section 2 of this article as well as discussed in some detail on why SVM results in being a large margin classifier in sections 1 through 4 above.&nbsp;</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4E12AQGy6cupLwQWow" src="https://media.licdn.com/dms/image/D4E12AQGy6cupLwQWow/article-inline_image-shrink_400_744/0/1653944576928?e=1691625600&amp;v=beta&amp;t=oozBrYTQpWWjMTeLUs6bOOm0qYLsH33DFCvjcoPqN1Q"></div><p><strong>                                     Figure: SVM with Linear Decision Boundary</strong></p><p>However, in the real world – a linear decision boundary is rarely the case (because the real world is non-linear) – that is: our decision boundary might well become nonlinear as shown in the figure below.</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4E12AQGoHZp9yX0SmQ" src="https://media.licdn.com/dms/image/D4E12AQGoHZp9yX0SmQ/article-inline_image-shrink_400_744/0/1653944681202?e=1691625600&amp;v=beta&amp;t=PyFnHcvK9oFeM2sB6axMIoSpQdiRoLU3T0lkfni0_Yw"></div><p><br></p><h2><strong>                             Figure: A Non-Linear Decision Boundary</strong></h2><p>One might now think that in order to accommodate a non-linear decision boundary we can formulate some complex polynomial features as shown below:</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4E12AQFDSAJcd7p8_A" src="https://media.licdn.com/dms/image/D4E12AQFDSAJcd7p8_A/article-inline_image-shrink_400_744/0/1653944766611?e=1691625600&amp;v=beta&amp;t=4658LxMeM6HmA2D1gwct7lg41QDPZ7KTgizT-KEFJ9g"></div><p>So that the hypothesis will be:</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4E12AQEc-stX1P4q7Q" src="https://media.licdn.com/dms/image/D4E12AQEc-stX1P4q7Q/article-inline_image-shrink_400_744/0/1653944806679?e=1691625600&amp;v=beta&amp;t=MeTLjIglIdR--j2Ujy6BhUpHmdyrXVFFlakT-VsIM20"></div><p>As one may expect, using such complex high order polynomials can get computationally expensive. So, what we do is we transform the problem into a new space so that in the new space the decision boundary is linear. For example, if we have a set of datapoints | nonlinear data as shown below:&nbsp;</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4E12AQELjbmGlYn3Iw" src="https://media.licdn.com/dms/image/D4E12AQELjbmGlYn3Iw/article-inline_image-shrink_400_744/0/1653944926779?e=1691625600&amp;v=beta&amp;t=BxaS5LLrnpe2TLgcHuoOfW0VRb3dgf108T6b6WB8KOM"></div><p><strong>                                                 Figure: Example of Non-Linear Data</strong></p><p><strong>Figure credits: </strong><a href="https://towardsdatascience.com/svm-and-kernel-svm-fed02bef1200" target="_blank"><strong>https://towardsdatascience.com/svm-and-kernel-svm-fed02bef1200</strong></a></p><p>And if we express the observations in a different space which can be mapped from x1 and x2 as:</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4E12AQGBhunU94W0dg" src="https://media.licdn.com/dms/image/D4E12AQGBhunU94W0dg/article-inline_image-shrink_400_744/0/1653945008897?e=1691625600&amp;v=beta&amp;t=FYoA1yRioWZjJ5EdC_7XSaFCGNJO8IL6ioazPb6Mkp0"></div><p>We can then notice that in the new space the data is linearly separable as shown below:</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4E12AQGQ7BPt3hi2qQ" src="https://media.licdn.com/dms/image/D4E12AQGQ7BPt3hi2qQ/article-inline_image-shrink_400_744/0/1653945103133?e=1691625600&amp;v=beta&amp;t=X3I59Ax4H8zqI1Osm3BuvXIKJpSYU3XCX9d8ThZ8SsY"></div><p><strong>Figure: Transformation of the problem into a different space where the classes are linearly separable</strong></p><p>Figure credits: <a href="https://towardsdatascience.com/svm-and-kernel-svm-fed02bef1200" target="_blank">https://towardsdatascience.com/svm-and-kernel-svm-fed02bef1200</a></p><p>After the transformation, the classes are linear separable. Such transformations are called “Kernels”. Popular kernels are: Polynomial kernel, Gaussian Kernel, Radial Basis Function (RBF), etc. Gaussian Kernels are most frequently employed and are discussed in some detail in section 6 below.&nbsp;</p><h2>6.&nbsp;&nbsp;Kernels | Gaussian Kernel</h2><p>&nbsp;So, we have discussed an intuitive explanation towards the definition of Kernels and how they conceptually help us whilst solving a problem with a nonlinear decision boundary in section 5 above. Let us now formalize the overall concept of Kernels and understand Kernels more formally!</p><p><br></p><h2>6.1&nbsp;&nbsp;What are Kernels? What is the Kernel Trick?</h2><p>&nbsp;Kernel Methods is a class of algorithms for pattern analysis – the general task of a pattern analysis is to find and study the general types of relations for example: clusters, rankings, principal components, correlations, rankings, etc in datasets. Kernel methods own their name to Kernel Functions which enable them to operate in higher dimension implicit feature space without ever computing the coordinates of that space but instead computing the inner product between pairs of data in that space. The operation is cheaper than the explicit computation of the coordinates in the higher dimension space. The operation is often referred to as: Kernel Trick!</p><p><br></p><h2>6.2&nbsp; Mathematical definition of the Gaussian Kernel:</h2><p>&nbsp;</p><p>Given 2Dimensional vectors xi and xj, the Gaussian Kernel is defined as:</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4E12AQEjsnvytW7YRw" src="https://media.licdn.com/dms/image/D4E12AQEjsnvytW7YRw/article-inline_image-shrink_1000_1488/0/1653945372373?e=1691625600&amp;v=beta&amp;t=OTPMRbXMTGppLoZHwklUkRMO5aX42gub4FeyWVtdBzM"></div><h2><br></h2><h2>6.3&nbsp;Computing new features given x using the Gaussian Kernel definition</h2><p>&nbsp;</p><p>The terminology of Gaussian Kernel and Lagrangian multipliers is often used interchangeably but both mean the same thing. As mentioned in section 5, we transform the problem into another space where the data is linearly separable and solve the problem in that space. As highlighted in section 6, the Kernel methods help us to operate in higher dimension space without having the need to compute the coordinates of the data in those dimensions. </p><p>Thus, with Gaussian Kernels, we compute the new features depending on the proximity to landmarks l(1), l(2), l(3)</p><p>To do this, we find the similarity of x and some landmark l(i) – the vectors xi and xj referred to in section 6.2 are taken as the given feature vector x and landmark l(i)</p><p>The similarity function is called the “Gaussian Kernel”. The similarity function is written as:</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4E12AQHEIXA-n7uu3g" src="https://media.licdn.com/dms/image/D4E12AQHEIXA-n7uu3g/article-inline_image-shrink_400_744/0/1653945431870?e=1691625600&amp;v=beta&amp;t=l7Q_ScnRqSRtw909_UGpfHa4vtNYzcu1x6kfQ7zj_Ak"></div><p><strong>Properties of the Similarity Function:</strong></p><p>1)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The Gaussian Kernel is a measure of similarity between xi and xj. It evaluates to 1 when x ~ l(i). That is:</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4E12AQERG25xz4ZHzw" src="https://media.licdn.com/dms/image/D4E12AQERG25xz4ZHzw/article-inline_image-shrink_400_744/0/1653945475551?e=1691625600&amp;v=beta&amp;t=VXpu444IYhi9AS_LSxRYBny1yWxB4Y4IIwxev4-joqA"></div><p>2)   If x is far from l(i), then,&nbsp;</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4E12AQFRoi2HwJ1hQw" src="https://media.licdn.com/dms/image/D4E12AQFRoi2HwJ1hQw/article-inline_image-shrink_400_744/0/1653945516142?e=1691625600&amp;v=beta&amp;t=EAmA-VYBxqDdsyWIYmfJgBNzNhrTzn5kmrzEs_jZ6ks"></div><p>Thus,</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If x and the landmark are close, then, the similarity function will be close to 1</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If x and the landmark are far away from each other then the similarity function will be close to 0</p><p>With the new features evaluated, the hypothesis function is written as:</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4E12AQEYNaCne2ID9g" src="https://media.licdn.com/dms/image/D4E12AQEYNaCne2ID9g/article-inline_image-shrink_400_744/0/1653945560715?e=1691625600&amp;v=beta&amp;t=HugwZShAvMsgYIhlpgJ5DUe1NzOdkqALSflb_AJaTdY"></div><h2>6.4&nbsp;How to select the landmark functions?</h2><p>&nbsp;</p><p>One way to select the landmarks is to put them in the exact same location as the training examples. Thus, if we have “m” training examples, we’ll get “m” landmarks with 1 landmark per training example.</p><p>Given example x,</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4E12AQE2bj5CtpAzOQ" src="https://media.licdn.com/dms/image/D4E12AQE2bj5CtpAzOQ/article-inline_image-shrink_400_744/0/1653945616611?e=1691625600&amp;v=beta&amp;t=HoX7ie84SdThoGS5wyJlrE81NLj3ox60RgWDQvl8qrE"></div><p>To get the parameters θ we use the same minimization function as discussed in section 2 but features f(i) instead of x(i)</p><p>That is:</p><div class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4E12AQFPqvGE_uIe_A" src="https://media.licdn.com/dms/image/D4E12AQFPqvGE_uIe_A/article-inline_image-shrink_400_744/0/1653945653781?e=1691625600&amp;v=beta&amp;t=SmWAKEWx6zCqI0-W1YVQMdQsoxFrswtCntGL6-9a8yg"></div><p>Note: Using Kernels to generate f(i) is not exclusive to SVMs and may also be applied to logistic regression. However, because of the computational optimizations in SVMs, kernels combined with SVMs are much faster than with other algorithms</p><p>&nbsp;</p><h2>7.&nbsp;&nbsp;&nbsp;Practical applications of SVM</h2><p>&nbsp;In practical scenario’s one has to make the following choices whilst using SVM:</p><p>a)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Choice of the parameter C: This is elaborated in section 3.</p><p>b)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Choice of “Kernel”</p><p>&nbsp;</p><ul><li><strong>Linear Kernel</strong> (or otherwise termed as No Kernel): Normally when the number of features is large (“n” is large) and the number of training examples (“m”) are small, then, obviously a nonlinear decision boundary may result in overfitting. Hence, one might opt for a linear kernel – linear decision boundary</li><li><strong>Gaussian Kernel</strong>: One normally may opt for “Gaussian Kernel” if number of training examples “m” are large, and “n” is small. </li></ul><p>&nbsp;</p><p>Having opted for Gaussian Kernel, one has to select σ2 – the bandwidth of the Kernel. Choosing σ2 one will have to carry out the bias-variance trade off. When σ2 is large we will have a higher bias and low variance and when σ2 is small we have a low bias and high variance</p><p>&nbsp;Other choices of Kernels:</p><p>It should be underscored that not all similarity functions are valid kernels. They must satisfy Mercer’s theorem which will guarantee that SVM carries out optimization correctly and does not diverge. Details of Mercer’s theorem: <a href="https://ai-master.gitbooks.io/kernel-svm/content/mercers-theorem.html" target="_blank">https://ai-master.gitbooks.io/kernel-svm/content/mercers-theorem.html</a></p><p>&nbsp;</p><p><br></p><p><br></p><p>.....To be continued..watch this space</p></div>
</body>
</html>